{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgvb1899/GCA-2/blob/Development/ActivationFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "L-jecG8FwNDk",
        "colab_type": "code",
        "outputId": "b1c2e910-4ae0-400e-a68d-68f79d4e9c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nAWA7WYrwk33",
        "colab_type": "code",
        "outputId": "ed5aaf3d-d0db-42d9-a030-c94948af831f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "f = h5py.File('/content/drive/My Drive/images.h5', 'r')\n",
        "\n",
        "images_train = f['Train/images'][...]\n",
        "labels_train = f['Train/labels'][...]\n",
        "\n",
        "images_test = f['Test/images'][...]\n",
        "labels_test = f['Test/labels'][...]\n",
        "\n",
        "f.close()\n",
        "\n",
        "num_train = images_train.shape[0]\n",
        "images_train_flatten = images_train.flatten().reshape(num_train, 100*100*3)\n",
        "print(\"Number Training Images: \", num_train)\n",
        "print(\"Shape of Flattened Training Images Array: \", images_train_flatten.shape)\n",
        "\n",
        "num_test = images_test.shape[0]\n",
        "images_test_flatten = images_test.flatten().reshape(num_test, 100*100*3)\n",
        "print(\"Number Testing Images: \", num_test)\n",
        "print(\"Shape of Flattened Testing Images Array: \", images_test_flatten.shape)\n",
        "\n",
        "train_set_x = images_train_flatten/255.\n",
        "test_set_x = images_test_flatten/255."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number Training Images:  4405\n",
            "Shape of Flattened Training Images Array:  (4405, 30000)\n",
            "Number Testing Images:  1617\n",
            "Shape of Flattened Testing Images Array:  (1617, 30000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g4AurOT7wNtN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A possible improvement to the model could be the use of a new activation function. After some research, we got the idea to use the hyperbolic tangent function, and the ReLU function"
      ]
    },
    {
      "metadata": {
        "id": "CcMK2czhw3Nq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tanh_activation(z):\n",
        "  \n",
        "  s = np.tanh(z)\n",
        "  \n",
        "  return s\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPN9O1Ob_OBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "  if z.any() < 0:\n",
        "    s = 0\n",
        "  elif z.all() >= 0:\n",
        "    s = z\n",
        "    \n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MbWzDfy5ejcv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To ensure the model works with the chosen function, all instances of the activation function must be written with the proper function, and the prediction labels must be changed to reflect the function. As the code is currently written, it will work with the tanh function. "
      ]
    },
    {
      "metadata": {
        "id": "c3Jq3WIwxKSG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_model(dim):\n",
        "\n",
        "    \n",
        "    w = np.zeros(shape=(dim, 1), dtype=float)\n",
        "    b = 0\n",
        "    \n",
        "    return w,b\n",
        "  \n",
        "def forward_propagate(X, Y, w, b):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns array of activations (one per image)\n",
        "    and cost (scalar).\n",
        "    \n",
        "    input:  X     array of flattened images (num_pixels,num_images)\n",
        "            Y     array of labels, 1 or 0 (num_images)\n",
        "            w     array of weights (num_pixels,1)\n",
        "            b     scalar bias (float)\n",
        "    output: A     array of activations (num_images)\n",
        "            cost  loss (float)\n",
        "    \"\"\"\n",
        "  \n",
        "    # get number of images under consideration\n",
        "    num_images = X.shape[1]\n",
        "  \n",
        "    # calculate the activation for each image\n",
        "    A = tanh_activation(np.dot(w.T, X) + b) # ACTIVATION FUNCTION HERE\n",
        "  \n",
        "    # calculate the cost (scalar) using cross-entropy\n",
        "    cost = np.squeeze( (-1. / num_images) * np.sum((Y*np.log(A))+(1-Y)*np.log(1-A)),axis=0) \n",
        "  \n",
        "    return A, cost    \n",
        "  \n",
        "def backward_propagate(X, Y, A):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns a dictionary of derivatives of cost function.\n",
        "    \n",
        "    input:  X      array of flattened images (num_pixels,num_images)\n",
        "            Y      array of labels, 1 or 0 (num_images)\n",
        "            A      array of activations (num_images)\n",
        "    output: grads  dictionary with keys dw and db\n",
        "    \"\"\"\n",
        "    \n",
        "    # get number of images under consideration\n",
        "    num_images = X.shape[1]\n",
        "  \n",
        "    # derivative of cost function wrt w (scalar)\n",
        "    dw = (1./num_images)*np.dot(X,((A-Y).T))\n",
        "    \n",
        "    # derivative of cost function wrt b (scalar)\n",
        "    db = (1./num_images)*np.sum(A-Y,axis=1)\n",
        "  \n",
        "    # create dictionary of derivatives (gradients)\n",
        "    grads = {\"dw\": dw, \"db\": db}\n",
        "  \n",
        "    return grads  \n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \n",
        "    \"\"\"\n",
        "    Optimize array of weights and scalar bias.\n",
        "    \n",
        "    input:  w               array of weights (num_pixels,1)\n",
        "            b               scalar bias (float)\n",
        "            X               array of flattened images (num_pixels,num_images)\n",
        "            Y               array of labels, 1 or 0 (num_images)\n",
        "            num_iterations  number of iterations for optimization (scalar)\n",
        "            learning_rate   gradient multiplier (scalar)\n",
        "            print_cost      boolean controlling user feedback\n",
        "    output: params          w and b after num_iterations of optimization\n",
        "            grads           dictionary with keys dw and db\n",
        "            costs           history of cost during optimization (list)\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    # iterate\n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        # forward propagation\n",
        "        A, cost = forward_propagate(X, Y, w, b)\n",
        "        \n",
        "        # backward propagation\n",
        "        grads = backward_propagate(X, Y, A)\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update array of weights and scalar bias\n",
        "        w = w - learning_rate*dw\n",
        "        b = b -  learning_rate*db\n",
        "        \n",
        "        # save the costs (every 100th)\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 10 training examples\n",
        "        if print_cost:\n",
        "          if i % 10 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    # save optimized w and b in dictionary\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    # save dw and db\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "  \n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Given a set of flattened images, predict their labels.\n",
        "    \n",
        "    input:   w               array of weights (num_pixels,1)\n",
        "             b               scalar bias (float)\n",
        "             X               array of flattened images (num_pixels,num_images)\n",
        "    output:  Y_prediction    array of predictions (num_images)\n",
        "    '''\n",
        "    \n",
        "    # get number of images\n",
        "    num_images = X.shape[1]\n",
        "    \n",
        "    # initialize prediction array\n",
        "    Y_prediction = np.zeros((1,num_images))\n",
        "    \n",
        "    # calculate activation (probability) for each image\n",
        "    A = tanh_activation(np.dot(w.T, X) + b) # ACTIVATION FUNCTION HERE\n",
        "    \n",
        "    # make predictions\n",
        "    Y_prediction[A>0] = 1 # Labels must reflect activation function here\n",
        "    Y_prediction[A<=0] = 0\n",
        "    \n",
        "    return Y_prediction\n",
        "  \n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    A wrapper for gradient descent.\n",
        "    \n",
        "    input:   X_train         array of flattened images for training (num_pixels,num_train)\n",
        "             Y_train         array of training labels (num_train)\n",
        "             X_test          array of flattened images for testing (num_pixels,num_test)\n",
        "             Y_test          array of testing labels (num_test)\n",
        "             num_iterations  number of iterations for optimization (scalar)\n",
        "             learning_rate   gradient multiplier (scalar)\n",
        "             print_cost      boolean controlling user feedback\n",
        "    output:  d               a dictionary of parameters and costs\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize\n",
        "    w, b = initialize_model(X_train.shape[0])\n",
        "    \n",
        "    # gradient descent with training set (optimization)\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # get w and b\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # predict on testing and training set\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # print errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    # save some parameters into a dictionary\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-ngUuUEx5o_",
        "colab_type": "code",
        "outputId": "148eb0fd-8425-47cc-8392-f2ed30cca8f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "cell_type": "code",
      "source": [
        "d = model(train_set_x.T, labels_train, test_set_x.T, labels_test, num_iterations = 200, learning_rate = 0.002, print_cost = True)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 10: nan\n",
            "Cost after iteration 20: nan\n",
            "Cost after iteration 30: 3.191071\n",
            "Cost after iteration 40: nan\n",
            "Cost after iteration 50: nan\n",
            "Cost after iteration 60: nan\n",
            "Cost after iteration 70: 1.434102\n",
            "Cost after iteration 80: 2.223786\n",
            "Cost after iteration 90: nan\n",
            "Cost after iteration 100: 1.751306\n",
            "Cost after iteration 110: 3.880849\n",
            "Cost after iteration 120: nan\n",
            "Cost after iteration 130: nan\n",
            "Cost after iteration 140: nan\n",
            "Cost after iteration 150: 1.829955\n",
            "Cost after iteration 160: nan\n",
            "Cost after iteration 170: nan\n",
            "Cost after iteration 180: nan\n",
            "Cost after iteration 190: 1.807058\n",
            "train accuracy: 74.91486946651531 %\n",
            "test accuracy: 31.787260358688926 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}