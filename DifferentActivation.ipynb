{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DifferentActivation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgvb1899/GCA-2/blob/Development/DifferentActivation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "L-jecG8FwNDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3de10e4-0fd4-4e47-b9a5-ebe951edb656"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nAWA7WYrwk33",
        "colab_type": "code",
        "outputId": "30258201-2a32-4be0-acc5-ec9b42db41ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "f = h5py.File('/content/drive/My Drive/images.h5', 'r')\n",
        "\n",
        "images_train = f['Train/images'][...]\n",
        "labels_train = f['Train/labels'][...]\n",
        "\n",
        "images_test = f['Test/images'][...]\n",
        "labels_test = f['Test/labels'][...]\n",
        "\n",
        "f.close()\n",
        "\n",
        "num_train = images_train.shape[0]\n",
        "images_train_flatten = images_train.flatten().reshape(num_train, 100*100*3)\n",
        "print(\"Number Training Images: \", num_train)\n",
        "print(\"Shape of Flattened Training Images Array: \", images_train_flatten.shape)\n",
        "\n",
        "num_test = images_test.shape[0]\n",
        "images_test_flatten = images_test.flatten().reshape(num_test, 100*100*3)\n",
        "print(\"Number Testing Images: \", num_test)\n",
        "print(\"Shape of Flattened Testing Images Array: \", images_test_flatten.shape)\n",
        "\n",
        "train_set_x = images_train_flatten/255.\n",
        "test_set_x = images_test_flatten/255."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number Training Images:  4405\n",
            "Shape of Flattened Training Images Array:  (4405, 30000)\n",
            "Number Testing Images:  1617\n",
            "Shape of Flattened Testing Images Array:  (1617, 30000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g4AurOT7wNtN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A possible improvement to the model could be the use of a new activation function. After some research, we got the idea to use the hyperbolic tangent function. "
      ]
    },
    {
      "metadata": {
        "id": "CcMK2czhw3Nq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tanh_activation(z):\n",
        "  \n",
        "  s = np.tanh(z)\n",
        "  \n",
        "  return s\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPN9O1Ob_OBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "  s = max(0,z.any())\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c3Jq3WIwxKSG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_model(dim):\n",
        "\n",
        "    \n",
        "    w = np.zeros(shape=(dim, 1), dtype=float)\n",
        "    b = 0\n",
        "    \n",
        "    return w,b\n",
        "  \n",
        "def forward_propagate(X, Y, w, b):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns array of activations (one per image)\n",
        "    and cost (scalar).\n",
        "    \n",
        "    input:  X     array of flattened images (num_pixels,num_images)\n",
        "            Y     array of labels, 1 or 0 (num_images)\n",
        "            w     array of weights (num_pixels,1)\n",
        "            b     scalar bias (float)\n",
        "    output: A     array of activations (num_images)\n",
        "            cost  loss (float)\n",
        "    \"\"\"\n",
        "  \n",
        "    # get number of images under consideration\n",
        "    num_images = X.shape[1]\n",
        "  \n",
        "    # calculate the activation for each image\n",
        "    A = relu_activation(np.dot(w.T, X) + b)\n",
        "  \n",
        "    # calculate the cost (scalar) using cross-entropy\n",
        "    cost = np.squeeze( (-1. / num_images) * np.sum((Y*np.log(A))+(1-Y)*np.log(1-A)),axis=0) \n",
        "  \n",
        "    return A, cost    \n",
        "  \n",
        "def backward_propagate(X, Y, A):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns a dictionary of derivatives of cost function.\n",
        "    \n",
        "    input:  X      array of flattened images (num_pixels,num_images)\n",
        "            Y      array of labels, 1 or 0 (num_images)\n",
        "            A      array of activations (num_images)\n",
        "    output: grads  dictionary with keys dw and db\n",
        "    \"\"\"\n",
        "    \n",
        "    # get number of images under consideration\n",
        "    num_images = X.shape[1]\n",
        "  \n",
        "    # derivative of cost function wrt w (scalar)\n",
        "    dw = (1./num_images)*np.dot(X,((A-Y).T))\n",
        "    \n",
        "    # derivative of cost function wrt b (scalar)\n",
        "    db = (1./num_images)*np.sum(A-Y,axis=1)\n",
        "  \n",
        "    # create dictionary of derivatives (gradients)\n",
        "    grads = {\"dw\": dw, \"db\": db}\n",
        "  \n",
        "    return grads  \n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \n",
        "    \"\"\"\n",
        "    Optimize array of weights and scalar bias.\n",
        "    \n",
        "    input:  w               array of weights (num_pixels,1)\n",
        "            b               scalar bias (float)\n",
        "            X               array of flattened images (num_pixels,num_images)\n",
        "            Y               array of labels, 1 or 0 (num_images)\n",
        "            num_iterations  number of iterations for optimization (scalar)\n",
        "            learning_rate   gradient multiplier (scalar)\n",
        "            print_cost      boolean controlling user feedback\n",
        "    output: params          w and b after num_iterations of optimization\n",
        "            grads           dictionary with keys dw and db\n",
        "            costs           history of cost during optimization (list)\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    # iterate\n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        # forward propagation\n",
        "        A, cost = forward_propagate(X, Y, w, b)\n",
        "        \n",
        "        # backward propagation\n",
        "        grads = backward_propagate(X, Y, A)\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update array of weights and scalar bias\n",
        "        w = w - learning_rate*dw\n",
        "        b = b -  learning_rate*db\n",
        "        \n",
        "        # save the costs (every 100th)\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 10 training examples\n",
        "        if print_cost:\n",
        "          if i % 10 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    # save optimized w and b in dictionary\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    # save dw and db\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "  \n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Given a set of flattened images, predict their labels.\n",
        "    \n",
        "    input:   w               array of weights (num_pixels,1)\n",
        "             b               scalar bias (float)\n",
        "             X               array of flattened images (num_pixels,num_images)\n",
        "    output:  Y_prediction    array of predictions (num_images)\n",
        "    '''\n",
        "    \n",
        "    # get number of images\n",
        "    num_images = X.shape[1]\n",
        "    \n",
        "    # initialize prediction array\n",
        "    Y_prediction = np.zeros((1,num_images))\n",
        "    \n",
        "    # calculate activation (probability) for each image\n",
        "    A = relu_activation(np.dot(w.T, X) + b)\n",
        "    \n",
        "    # make predictions\n",
        "    Y_prediction[A>0] = 1\n",
        "    Y_prediction[A==0] = 0\n",
        "    \n",
        "    return Y_prediction\n",
        "  \n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    A wrapper for gradient descent.\n",
        "    \n",
        "    input:   X_train         array of flattened images for training (num_pixels,num_train)\n",
        "             Y_train         array of training labels (num_train)\n",
        "             X_test          array of flattened images for testing (num_pixels,num_test)\n",
        "             Y_test          array of testing labels (num_test)\n",
        "             num_iterations  number of iterations for optimization (scalar)\n",
        "             learning_rate   gradient multiplier (scalar)\n",
        "             print_cost      boolean controlling user feedback\n",
        "    output:  d               a dictionary of parameters and costs\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize\n",
        "    w, b = initialize_model(X_train.shape[0])\n",
        "    \n",
        "    # gradient descent with training set (optimization)\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # get w and b\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # predict on testing and training set\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # print errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    # save some parameters into a dictionary\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-ngUuUEx5o_",
        "colab_type": "code",
        "outputId": "3e6a05a4-ec14-4d23-b6eb-f22fb1332f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "cell_type": "code",
      "source": [
        "d = model(train_set_x.T, labels_train, test_set_x.T, labels_test, num_iterations = 200, learning_rate = 0.002, print_cost = True)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-53195292c126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-ada2e8ad114f>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# gradient descent with training set (optimization)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# get w and b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-ada2e8ad114f>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(w, b, X, Y, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# backward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-ada2e8ad114f>\u001b[0m in \u001b[0;36mforward_propagate\u001b[0;34m(X, Y, w, b)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# calculate the activation for each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# calculate the cost (scalar) using cross-entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-29017e267740>\u001b[0m in \u001b[0;36mrelu_activation\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrelu_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cFQDF9yGCfbC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25af9d30-e273-4a67-9f71-755af48b87e7"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}