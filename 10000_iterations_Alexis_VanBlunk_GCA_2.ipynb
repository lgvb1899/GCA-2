{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10000 iterations Alexis VanBlunk - GCA_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgvb1899/GCA-2/blob/Development/10000_iterations_Alexis_VanBlunk_GCA_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LefCCwDeB3Nm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "XKoEENrWUX5j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GCA 2 will require approximately four hours of your time.  You may complete GCA 2 in Colab or locally on your machines.  Please post any code you develop to GitHub.  Make sure that each of your group members has access to your group repository.\n",
        "\n",
        "In GCA 2, you will code your own model to identify images of cats from other images.  Specifically, you will code a logistic regression classisier.  This assignment was inspired by Andrew Ng's Deep Learning course offered by deeplearning.ai, adapted from your instructor's own notes.  Your instructor assembled the data.  Images were taken from the Cats and Dogs dataset on Kaggle and Caltech 101 Image Dataset.\n",
        "\n",
        "Let's get started."
      ]
    },
    {
      "metadata": {
        "id": "K1eiqQnEB8_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1: Mount Google Drive and Download Images"
      ]
    },
    {
      "metadata": {
        "id": "FsisUbEuc186",
        "colab_type": "code",
        "outputId": "de4cc2f6-de1f-42d1-f1f0-02d41e1166a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kkT5AegfWN78",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download images from the following link:\n",
        "\n",
        "https://drive.google.com/open?id=1pNbSKJNl4SUVguMpIfK7UI9CtyubbS9M\n",
        "\n",
        "Place the file contaning the images (`images.h5`) in a convenient place in your Google Drive."
      ]
    },
    {
      "metadata": {
        "id": "AvGYVtveCCmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import Modules"
      ]
    },
    {
      "metadata": {
        "id": "Cmsb2emlYBmr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import the modules `h5py`, `numpy`, and `matplotlib`."
      ]
    },
    {
      "metadata": {
        "id": "kq39u35J-2Iy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z4VWXNAyCKtn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3:  Read Data File"
      ]
    },
    {
      "metadata": {
        "id": "Jd6aGOgXYQJ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read the HDF5 file containing the images.  An HDF5 file is a smart data container.  For our purposes now, the file contains multi-dimensional arrays related to our images.\n",
        "\n",
        "`Train/images`\n",
        "<br>\n",
        "Contains 4405 training images each with resolution 100 x 100.  It is a 4D NumPy array with shape `(num_images,100,100,3)`.  We read this data into the variable `images_train` below.\n",
        "\n",
        "`Test/images`\n",
        "<br>\n",
        "Contains 1617 testing images each with resolution 100 x 100.  It is a 4D NumPy array with shape `(num_images,100,100,3)`.  We read this data into the variable `images_test` below.\n",
        "\n",
        "`Train/labels`\n",
        "<br>\n",
        "A one-dimensional array with length equal to the number of training images.  It contains 1 or 0.  1 for cat.  0 for not cat.  The row in this array corresponds to the first index of `images_train`.  We read this data into the variable `labels_train` below.\n",
        "\n",
        "`Test/labels`\n",
        "<br>\n",
        "A one-dimensional array with length equal to the number of testing images.  It contains 1 or 0.  1 for cat.  0 for not cat.  The row in this array corresponds to the first index of `images_test`.  We read this data into the variable `labels_test` below."
      ]
    },
    {
      "metadata": {
        "id": "mMQXqZgLQQly",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from h5py import*\n",
        "f = File('/content/drive/My Drive/images_no_cats.h5', 'r')\n",
        "images_new = f['images'][...]\n",
        "images = []\n",
        "for i in range(4000):\n",
        "  images = images + [images_new[i]]\n",
        "labels = f['labels'][...]\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m0dsj7a4-WMw",
        "colab_type": "code",
        "outputId": "3d55622f-3248-486a-fcc8-dc90b8f6db59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "f = h5py.File('/content/drive/My Drive/images.h5', 'r')\n",
        "\n",
        "images_train = f['Train/images'][...]\n",
        "images_train = np.concatenate((images_train,images), axis = 0)\n",
        "labels_train = f['Train/labels'][...]\n",
        "zeros = np.zeros(4000)\n",
        "labels_train = np.concatenate((labels_train, zeros))\n",
        "\n",
        "images_test = f['Test/images'][...]\n",
        "labels_test = f['Test/labels'][...]\n",
        "\n",
        "f.close()\n",
        "print(np.shape(images_train))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8405, 100, 100, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qSmVGQLKCSLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4: Plot Some Data"
      ]
    },
    {
      "metadata": {
        "id": "3ctpBPKjb84a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's get a better understanding of what these arrays contain.  Let's plot a cat and its correpsonding label."
      ]
    },
    {
      "metadata": {
        "id": "gb0-noeEcFsp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = images_train[1000]\n",
        "label = labels_train[1000]\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "print('The label of this image is ', label, '.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGjLIm7vcj7Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look of an image without a cat."
      ]
    },
    {
      "metadata": {
        "id": "Co59Pan1cm7S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = images_train[3]\n",
        "label = labels_train[3]\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "print('The label of this image is ', label, '.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPplDIBWc9C0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice how the stapler has label 0.  This is because it is not a cat.  Only images with cats are labeled 1.  Also, notice that the images have been re-sized to have the same resolution.  This is necessary as our model will only accept RGB images of resolution 100 x 100.\n",
        "\n",
        "What is `image`?  `image` is an image, but in Python, its simply a NumPy array.  It's a 100 x 100 x 3 NumPy array.  100 x 100 is the image resolution.  The `3` indicates that the image in encoded in RGB.  `[:,:,0]` is the red channel.  `[:,:,1]` is the green channel.  Finally, `[:,:,2]` is the blue channel.\n",
        "\n",
        "Here is a better indication of the types of images in our dataset."
      ]
    },
    {
      "metadata": {
        "id": "-dFmNPWBBaEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(500,600):\n",
        "    plt.subplot(10, 10, i-499)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(images_test[i])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_neQGQICX98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 5: Flatten the Images"
      ]
    },
    {
      "metadata": {
        "id": "RJkg_uZ_eeyM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For our model, we'll need to flatten our images.  Remember, each image is `100 x 100 x 3`.  We will preserve each pixel, but require a different pixel arrangement.  We convert each three-dimension NumPy image array to a one-dimension array using the method `flatten`."
      ]
    },
    {
      "metadata": {
        "id": "o9zSKouz_knw",
        "colab_type": "code",
        "outputId": "4794c4bf-ed28-499f-bac6-7d58fb64976d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "num_train = images_train.shape[0]\n",
        "images_train_flatten = images_train.flatten().reshape(num_train, 100*100*3)\n",
        "print(\"Number Training Images: \", num_train)\n",
        "print(\"Shape of Flattened Training Images Array: \", images_train_flatten.shape)\n",
        "\n",
        "num_test = images_test.shape[0]\n",
        "images_test_flatten = images_test.flatten().reshape(num_test, 100*100*3)\n",
        "print(\"Number Testing Images: \", num_test)\n",
        "print(\"Shape of Flattened Testing Images Array: \", images_test_flatten.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number Training Images:  8405\n",
            "Shape of Flattened Training Images Array:  (8405, 30000)\n",
            "Number Testing Images:  1617\n",
            "Shape of Flattened Testing Images Array:  (1617, 30000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vce-ZwhBDeqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 6: Standardize the Data"
      ]
    },
    {
      "metadata": {
        "id": "tenS_NfBfV2j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Standardizing data is a very important step when creating machine leanring models.  What happens if our photos of cats were taken at different exposures?  We'd have to compensate for this, otherwise we'd might \"train\" our model to discriminate different exposures when classifying cats.  This is a waste and could compromise the accuracy of our model!  Instead, we normalize our images based on the highest permissible pixel value, 255."
      ]
    },
    {
      "metadata": {
        "id": "xQ_2SRSIDck9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_set_x = images_train_flatten/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3F0O21WNvaT-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_set_x = images_test_flatten/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zep1JnGiE37p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 7: Assembling the Model"
      ]
    },
    {
      "metadata": {
        "id": "NGcQ_r2igdcb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following image is from Andrew Ng's course on Deep Learning.  What are we looking at?  Basically, this is our predictor.  It accepts three imputs: a scalar bias, an array of weights with length equal to the total number of pixels in a given image (3x100x100), and flattened image(s).  You will learn how to train this predictor for improved identification.  Effectively, your goal is to settle on an optimized scalar bias and array of weights."
      ]
    },
    {
      "metadata": {
        "id": "7k1d9H8LgXDz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/andersy005/deep-learning-specialization-coursera/raw/8fd8ad74a9b2d8321bb0a701c5c0f3ec4abad144/01-Neural-Networks-and-Deep-Learning/week2/Programming-Assignments/images/LogReg_kiank.png)"
      ]
    },
    {
      "metadata": {
        "id": "EXOompHimx2T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Working from left to right, an image is flattened and normalized (we've already done this).  Each pixel is multiplied by its corresponding weight.  The products are summed.  The scalar bias is then added to this summation.  This result, in turn, is pushed through the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function).  The sigmoid function is known as an activation function and outputs a scalar between 0 and 1.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*Xu7B5y9gp0iL5ooBj7LtWw.png)\n",
        "\n",
        "Once our model is sufficiently trained, you can think of this scalar as a probability that the image contains a cat.  For example, if the \"probability\" is greater than 0.5, we classify the image as cat."
      ]
    },
    {
      "metadata": {
        "id": "arVQzQnboikO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start coding pieces of the model."
      ]
    },
    {
      "metadata": {
        "id": "j7Or4nBFE_Yv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sigmoid Function"
      ]
    },
    {
      "metadata": {
        "id": "e05cW1Q_FCf6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  \n",
        "    \"\"\"\n",
        "    Computes the sigmoid function.\n",
        "    \n",
        "    input:  scalar or numpy array\n",
        "    output: scalar or numpy array\n",
        "    \"\"\"\n",
        "\n",
        "    s = 1. / ( 1 + np.exp(-z))\n",
        "    \n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LsY7_zb2FekF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Weights and Biases\n",
        "To initialize the model, we need to zero-out our weight array and scalar bias.  Recall that each pixel is assigned its own weight."
      ]
    },
    {
      "metadata": {
        "id": "fLHbLozyFhet",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_model(dim):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns initialized weight array and bias scalar.\n",
        "    \n",
        "    input:  number of weights\n",
        "    output: weights (array), biases (scalar)\n",
        "    \"\"\"\n",
        "    \n",
        "    w = np.zeros(shape=(dim, 1), dtype=float)\n",
        "    b = 0\n",
        "    \n",
        "    return w,b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oKeDB96cGgLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 8: Training the Model\n",
        "Training the model consists of iteratively determining an optimized set of input parameters.  These parameters are the weights and scalar bias.\n",
        "\n",
        "We first define forward propagation.  Forward propagation consists of exercising the weights and bias on all available training images, calculating an activation from the sigmoid function for each, and finally calculating a cost (or loss) associated with our \"predictions\".  We use the cross-entropy cost.  Please read [here](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#cost-function) for more details.  Regardless of number of images, cost will always be a scalar.  The higher the cost, the worse our predictions."
      ]
    },
    {
      "metadata": {
        "id": "lbnh3YA3Gqyw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_propagate(X, Y, w, b):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns array of activations (one per image)\n",
        "    and cost (scalar).\n",
        "    \n",
        "    input:  X     array of flattened images (num_pixels,num_images)\n",
        "            Y     array of labels, 1 or 0 (num_images)\n",
        "            w     array of weights (num_pixels,1)\n",
        "            b     scalar bias (float)\n",
        "    output: A     array of activations (num_images)\n",
        "            cost  loss (float)\n",
        "    \"\"\"\n",
        "  \n",
        "    # get number of images under consideration\n",
        "    num_images = X.shape[1]\n",
        "  \n",
        "    # calculate the activation for each image\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "  \n",
        "    # calculate the cost (scalar) using cross-entropy\n",
        "    cost = np.squeeze( (-1. / num_images) * np.sum((Y*np.log(A)+(1-Y)*np.log(1-A)),axis=1) )\n",
        "  \n",
        "    return A, cost  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VMvMovQYv7Qs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We propagate backwards to minimize our cost.  This involves taking derivatives of the cost function.  We take derivatives in terms of our weights and scalar bias.  For more info, read [here](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#gradient-descent).  Without this function, our optimization routine is aimless."
      ]
    },
    {
      "metadata": {
        "id": "-wamExdGKMpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backward_propagate(X, Y, A):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns a dictionary of derivatives of cost function.\n",
        "    \n",
        "    input:  X      array of flattened images (num_pixels,num_images)\n",
        "            Y      array of labels, 1 or 0 (num_images)\n",
        "            A      array of activations (num_images)\n",
        "    output: grads  dictionary with keys dw and db\n",
        "    \"\"\"\n",
        "    \n",
        "    # get number of images under consideration\n",
        "    num_images = X.shape[1]\n",
        "  \n",
        "    # derivative of cost function wrt w (scalar)\n",
        "    dw = (1./num_images)*np.dot(X,((A-Y).T))\n",
        "    \n",
        "    # derivative of cost function wrt b (scalar)\n",
        "    db = (1./num_images)*np.sum(A-Y,axis=1)\n",
        "  \n",
        "    # create dictionary of derivatives (gradients)\n",
        "    grads = {\"dw\": dw, \"db\": db}\n",
        "  \n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8YRAhYI50Azb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now assemble our optimization routine.  This routine is known as gradient descent.  It is an iterative process.  For each iteration, we conduct forward and backward propagation.  We update our array of weights and scalar bias by subtracting gradients from their previous values.  We apply a learning rate to this update.  If the learning rate is too high, our optimization procedure will not converge.  If the learning rate is too low, our optimization routine will converge to a solution, albeit slowly.  "
      ]
    },
    {
      "metadata": {
        "id": "vDSM-UzMLZyY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \n",
        "    \"\"\"\n",
        "    Optimize array of weights and scalar bias.\n",
        "    \n",
        "    input:  w               array of weights (num_pixels,1)\n",
        "            b               scalar bias (float)\n",
        "            X               array of flattened images (num_pixels,num_images)\n",
        "            Y               array of labels, 1 or 0 (num_images)\n",
        "            num_iterations  number of iterations for optimization (scalar)\n",
        "            learning_rate   gradient multiplier (scalar)\n",
        "            print_cost      boolean controlling user feedback\n",
        "    output: params          w and b after num_iterations of optimization\n",
        "            grads           dictionary with keys dw and db\n",
        "            costs           history of cost during optimization (list)\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    # iterate\n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        # forward propagation\n",
        "        A, cost = forward_propagate(X, Y, w, b)\n",
        "        \n",
        "        # backward propagation\n",
        "        grads = backward_propagate(X, Y, A)\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update array of weights and scalar bias\n",
        "        w = w - learning_rate*dw\n",
        "        b = b -  learning_rate*db\n",
        "        \n",
        "        # save the costs (every 100th)\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 10 training examples\n",
        "        if print_cost:\n",
        "          if i % 10 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    # save optimized w and b in dictionary\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    # save dw and db\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6jxlyZK54BBe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now code a function to actually do our prediciton.  Remember, an activation >= 0.5 is a cat.  Else, it's not a cat."
      ]
    },
    {
      "metadata": {
        "id": "ZVIisVKTO1MB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Given a set of flattened images, predict their labels.\n",
        "    \n",
        "    input:   w               array of weights (num_pixels,1)\n",
        "             b               scalar bias (float)\n",
        "             X               array of flattened images (num_pixels,num_images)\n",
        "    output:  Y_prediction    array of predictions (num_images)\n",
        "    '''\n",
        "    \n",
        "    # get number of images\n",
        "    num_images = X.shape[1]\n",
        "    \n",
        "    # initialize prediction array\n",
        "    Y_prediction = np.zeros((1,num_images))\n",
        "    \n",
        "    # calculate activation (probability) for each image\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    \n",
        "    # make predictions\n",
        "    Y_prediction[A>=0.5] = 1\n",
        "    Y_prediction[A<0.5] = 0\n",
        "    \n",
        "    return Y_prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Yv7hPXd4mu1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now wrap everything up conveniently in a single function that orchestrates the optimization and reports diagnostics."
      ]
    },
    {
      "metadata": {
        "id": "BDD8laDbPSvJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 1000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    A wrapper for gradient descent.\n",
        "    \n",
        "    input:   X_train         array of flattened images for training (num_pixels,num_train)\n",
        "             Y_train         array of training labels (num_train)\n",
        "             X_test          array of flattened images for testing (num_pixels,num_test)\n",
        "             Y_test          array of testing labels (num_test)\n",
        "             num_iterations  number of iterations for optimization (scalar)\n",
        "             learning_rate   gradient multiplier (scalar)\n",
        "             print_cost      boolean controlling user feedback\n",
        "    output:  d               a dictionary of parameters and costs\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize\n",
        "    w, b = initialize_model(X_train.shape[0])\n",
        "    \n",
        "    # gradient descent with training set (optimization)\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # get w and b\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # predict on testing and training set\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # print errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    # save some parameters into a dictionary\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "owLPoNWI6ERm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's run everything."
      ]
    },
    {
      "metadata": {
        "id": "VvoESatcPjNM",
        "colab_type": "code",
        "outputId": "6085376a-8d92-41fd-f63b-e787b1a5d668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18754
        }
      },
      "cell_type": "code",
      "source": [
        "d = model(train_set_x.T, labels_train, test_set_x.T, labels_test, num_iterations = 10000, learning_rate = 0.002, print_cost = True)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 10: 0.693859\n",
            "Cost after iteration 20: 0.664061\n",
            "Cost after iteration 30: 0.975437\n",
            "Cost after iteration 40: 0.665026\n",
            "Cost after iteration 50: 0.961135\n",
            "Cost after iteration 60: 0.674485\n",
            "Cost after iteration 70: 0.927890\n",
            "Cost after iteration 80: 0.694614\n",
            "Cost after iteration 90: 0.891936\n",
            "Cost after iteration 100: 0.720006\n",
            "Cost after iteration 110: 0.851959\n",
            "Cost after iteration 120: 0.748582\n",
            "Cost after iteration 130: 0.814113\n",
            "Cost after iteration 140: 0.770649\n",
            "Cost after iteration 150: 0.789634\n",
            "Cost after iteration 160: 0.777661\n",
            "Cost after iteration 170: 0.779076\n",
            "Cost after iteration 180: 0.775622\n",
            "Cost after iteration 190: 0.773822\n",
            "Cost after iteration 200: 0.771645\n",
            "Cost after iteration 210: 0.769639\n",
            "Cost after iteration 220: 0.767669\n",
            "Cost after iteration 230: 0.765758\n",
            "Cost after iteration 240: 0.763898\n",
            "Cost after iteration 250: 0.762086\n",
            "Cost after iteration 260: 0.760321\n",
            "Cost after iteration 270: 0.758598\n",
            "Cost after iteration 280: 0.756917\n",
            "Cost after iteration 290: 0.755274\n",
            "Cost after iteration 300: 0.753667\n",
            "Cost after iteration 310: 0.752095\n",
            "Cost after iteration 320: 0.750557\n",
            "Cost after iteration 330: 0.749049\n",
            "Cost after iteration 340: 0.747572\n",
            "Cost after iteration 350: 0.746123\n",
            "Cost after iteration 360: 0.744702\n",
            "Cost after iteration 370: 0.743307\n",
            "Cost after iteration 380: 0.741937\n",
            "Cost after iteration 390: 0.740592\n",
            "Cost after iteration 400: 0.739269\n",
            "Cost after iteration 410: 0.737969\n",
            "Cost after iteration 420: 0.736690\n",
            "Cost after iteration 430: 0.735432\n",
            "Cost after iteration 440: 0.734194\n",
            "Cost after iteration 450: 0.732976\n",
            "Cost after iteration 460: 0.731776\n",
            "Cost after iteration 470: 0.730594\n",
            "Cost after iteration 480: 0.729429\n",
            "Cost after iteration 490: 0.728281\n",
            "Cost after iteration 500: 0.727150\n",
            "Cost after iteration 510: 0.726035\n",
            "Cost after iteration 520: 0.724935\n",
            "Cost after iteration 530: 0.723850\n",
            "Cost after iteration 540: 0.722779\n",
            "Cost after iteration 550: 0.721723\n",
            "Cost after iteration 560: 0.720680\n",
            "Cost after iteration 570: 0.719651\n",
            "Cost after iteration 580: 0.718635\n",
            "Cost after iteration 590: 0.717631\n",
            "Cost after iteration 600: 0.716639\n",
            "Cost after iteration 610: 0.715660\n",
            "Cost after iteration 620: 0.714692\n",
            "Cost after iteration 630: 0.713736\n",
            "Cost after iteration 640: 0.712790\n",
            "Cost after iteration 650: 0.711856\n",
            "Cost after iteration 660: 0.710931\n",
            "Cost after iteration 670: 0.710017\n",
            "Cost after iteration 680: 0.709114\n",
            "Cost after iteration 690: 0.708219\n",
            "Cost after iteration 700: 0.707335\n",
            "Cost after iteration 710: 0.706459\n",
            "Cost after iteration 720: 0.705593\n",
            "Cost after iteration 730: 0.704736\n",
            "Cost after iteration 740: 0.703887\n",
            "Cost after iteration 750: 0.703047\n",
            "Cost after iteration 760: 0.702215\n",
            "Cost after iteration 770: 0.701391\n",
            "Cost after iteration 780: 0.700574\n",
            "Cost after iteration 790: 0.699766\n",
            "Cost after iteration 800: 0.698965\n",
            "Cost after iteration 810: 0.698172\n",
            "Cost after iteration 820: 0.697385\n",
            "Cost after iteration 830: 0.696606\n",
            "Cost after iteration 840: 0.695834\n",
            "Cost after iteration 850: 0.695068\n",
            "Cost after iteration 860: 0.694309\n",
            "Cost after iteration 870: 0.693556\n",
            "Cost after iteration 880: 0.692810\n",
            "Cost after iteration 890: 0.692070\n",
            "Cost after iteration 900: 0.691336\n",
            "Cost after iteration 910: 0.690608\n",
            "Cost after iteration 920: 0.689885\n",
            "Cost after iteration 930: 0.689169\n",
            "Cost after iteration 940: 0.688458\n",
            "Cost after iteration 950: 0.687752\n",
            "Cost after iteration 960: 0.687052\n",
            "Cost after iteration 970: 0.686357\n",
            "Cost after iteration 980: 0.685667\n",
            "Cost after iteration 990: 0.684982\n",
            "Cost after iteration 1000: 0.684303\n",
            "Cost after iteration 1010: 0.683628\n",
            "Cost after iteration 1020: 0.682957\n",
            "Cost after iteration 1030: 0.682292\n",
            "Cost after iteration 1040: 0.681631\n",
            "Cost after iteration 1050: 0.680974\n",
            "Cost after iteration 1060: 0.680322\n",
            "Cost after iteration 1070: 0.679675\n",
            "Cost after iteration 1080: 0.679031\n",
            "Cost after iteration 1090: 0.678392\n",
            "Cost after iteration 1100: 0.677757\n",
            "Cost after iteration 1110: 0.677125\n",
            "Cost after iteration 1120: 0.676498\n",
            "Cost after iteration 1130: 0.675875\n",
            "Cost after iteration 1140: 0.675256\n",
            "Cost after iteration 1150: 0.674640\n",
            "Cost after iteration 1160: 0.674028\n",
            "Cost after iteration 1170: 0.673420\n",
            "Cost after iteration 1180: 0.672815\n",
            "Cost after iteration 1190: 0.672214\n",
            "Cost after iteration 1200: 0.671616\n",
            "Cost after iteration 1210: 0.671021\n",
            "Cost after iteration 1220: 0.670430\n",
            "Cost after iteration 1230: 0.669843\n",
            "Cost after iteration 1240: 0.669258\n",
            "Cost after iteration 1250: 0.668677\n",
            "Cost after iteration 1260: 0.668099\n",
            "Cost after iteration 1270: 0.667524\n",
            "Cost after iteration 1280: 0.666952\n",
            "Cost after iteration 1290: 0.666383\n",
            "Cost after iteration 1300: 0.665817\n",
            "Cost after iteration 1310: 0.665254\n",
            "Cost after iteration 1320: 0.664694\n",
            "Cost after iteration 1330: 0.664136\n",
            "Cost after iteration 1340: 0.663582\n",
            "Cost after iteration 1350: 0.663030\n",
            "Cost after iteration 1360: 0.662481\n",
            "Cost after iteration 1370: 0.661934\n",
            "Cost after iteration 1380: 0.661390\n",
            "Cost after iteration 1390: 0.660849\n",
            "Cost after iteration 1400: 0.660311\n",
            "Cost after iteration 1410: 0.659775\n",
            "Cost after iteration 1420: 0.659241\n",
            "Cost after iteration 1430: 0.658710\n",
            "Cost after iteration 1440: 0.658181\n",
            "Cost after iteration 1450: 0.657655\n",
            "Cost after iteration 1460: 0.657131\n",
            "Cost after iteration 1470: 0.656610\n",
            "Cost after iteration 1480: 0.656090\n",
            "Cost after iteration 1490: 0.655573\n",
            "Cost after iteration 1500: 0.655059\n",
            "Cost after iteration 1510: 0.654546\n",
            "Cost after iteration 1520: 0.654036\n",
            "Cost after iteration 1530: 0.653528\n",
            "Cost after iteration 1540: 0.653022\n",
            "Cost after iteration 1550: 0.652518\n",
            "Cost after iteration 1560: 0.652017\n",
            "Cost after iteration 1570: 0.651517\n",
            "Cost after iteration 1580: 0.651019\n",
            "Cost after iteration 1590: 0.650524\n",
            "Cost after iteration 1600: 0.650030\n",
            "Cost after iteration 1610: 0.649539\n",
            "Cost after iteration 1620: 0.649049\n",
            "Cost after iteration 1630: 0.648562\n",
            "Cost after iteration 1640: 0.648076\n",
            "Cost after iteration 1650: 0.647592\n",
            "Cost after iteration 1660: 0.647110\n",
            "Cost after iteration 1670: 0.646630\n",
            "Cost after iteration 1680: 0.646152\n",
            "Cost after iteration 1690: 0.645675\n",
            "Cost after iteration 1700: 0.645200\n",
            "Cost after iteration 1710: 0.644727\n",
            "Cost after iteration 1720: 0.644256\n",
            "Cost after iteration 1730: 0.643787\n",
            "Cost after iteration 1740: 0.643319\n",
            "Cost after iteration 1750: 0.642853\n",
            "Cost after iteration 1760: 0.642389\n",
            "Cost after iteration 1770: 0.641926\n",
            "Cost after iteration 1780: 0.641465\n",
            "Cost after iteration 1790: 0.641006\n",
            "Cost after iteration 1800: 0.640548\n",
            "Cost after iteration 1810: 0.640092\n",
            "Cost after iteration 1820: 0.639638\n",
            "Cost after iteration 1830: 0.639185\n",
            "Cost after iteration 1840: 0.638734\n",
            "Cost after iteration 1850: 0.638284\n",
            "Cost after iteration 1860: 0.637836\n",
            "Cost after iteration 1870: 0.637389\n",
            "Cost after iteration 1880: 0.636944\n",
            "Cost after iteration 1890: 0.636500\n",
            "Cost after iteration 1900: 0.636057\n",
            "Cost after iteration 1910: 0.635617\n",
            "Cost after iteration 1920: 0.635177\n",
            "Cost after iteration 1930: 0.634739\n",
            "Cost after iteration 1940: 0.634303\n",
            "Cost after iteration 1950: 0.633868\n",
            "Cost after iteration 1960: 0.633434\n",
            "Cost after iteration 1970: 0.633002\n",
            "Cost after iteration 1980: 0.632571\n",
            "Cost after iteration 1990: 0.632141\n",
            "Cost after iteration 2000: 0.631713\n",
            "Cost after iteration 2010: 0.631286\n",
            "Cost after iteration 2020: 0.630861\n",
            "Cost after iteration 2030: 0.630436\n",
            "Cost after iteration 2040: 0.630014\n",
            "Cost after iteration 2050: 0.629592\n",
            "Cost after iteration 2060: 0.629172\n",
            "Cost after iteration 2070: 0.628753\n",
            "Cost after iteration 2080: 0.628335\n",
            "Cost after iteration 2090: 0.627918\n",
            "Cost after iteration 2100: 0.627503\n",
            "Cost after iteration 2110: 0.627089\n",
            "Cost after iteration 2120: 0.626676\n",
            "Cost after iteration 2130: 0.626265\n",
            "Cost after iteration 2140: 0.625854\n",
            "Cost after iteration 2150: 0.625445\n",
            "Cost after iteration 2160: 0.625037\n",
            "Cost after iteration 2170: 0.624630\n",
            "Cost after iteration 2180: 0.624225\n",
            "Cost after iteration 2190: 0.623820\n",
            "Cost after iteration 2200: 0.623417\n",
            "Cost after iteration 2210: 0.623014\n",
            "Cost after iteration 2220: 0.622613\n",
            "Cost after iteration 2230: 0.622213\n",
            "Cost after iteration 2240: 0.621815\n",
            "Cost after iteration 2250: 0.621417\n",
            "Cost after iteration 2260: 0.621020\n",
            "Cost after iteration 2270: 0.620625\n",
            "Cost after iteration 2280: 0.620230\n",
            "Cost after iteration 2290: 0.619837\n",
            "Cost after iteration 2300: 0.619445\n",
            "Cost after iteration 2310: 0.619054\n",
            "Cost after iteration 2320: 0.618663\n",
            "Cost after iteration 2330: 0.618274\n",
            "Cost after iteration 2340: 0.617886\n",
            "Cost after iteration 2350: 0.617499\n",
            "Cost after iteration 2360: 0.617113\n",
            "Cost after iteration 2370: 0.616728\n",
            "Cost after iteration 2380: 0.616344\n",
            "Cost after iteration 2390: 0.615961\n",
            "Cost after iteration 2400: 0.615579\n",
            "Cost after iteration 2410: 0.615198\n",
            "Cost after iteration 2420: 0.614818\n",
            "Cost after iteration 2430: 0.614439\n",
            "Cost after iteration 2440: 0.614061\n",
            "Cost after iteration 2450: 0.613684\n",
            "Cost after iteration 2460: 0.613308\n",
            "Cost after iteration 2470: 0.612933\n",
            "Cost after iteration 2480: 0.612559\n",
            "Cost after iteration 2490: 0.612185\n",
            "Cost after iteration 2500: 0.611813\n",
            "Cost after iteration 2510: 0.611442\n",
            "Cost after iteration 2520: 0.611071\n",
            "Cost after iteration 2530: 0.610702\n",
            "Cost after iteration 2540: 0.610333\n",
            "Cost after iteration 2550: 0.609965\n",
            "Cost after iteration 2560: 0.609598\n",
            "Cost after iteration 2570: 0.609232\n",
            "Cost after iteration 2580: 0.608867\n",
            "Cost after iteration 2590: 0.608503\n",
            "Cost after iteration 2600: 0.608140\n",
            "Cost after iteration 2610: 0.607777\n",
            "Cost after iteration 2620: 0.607415\n",
            "Cost after iteration 2630: 0.607055\n",
            "Cost after iteration 2640: 0.606695\n",
            "Cost after iteration 2650: 0.606336\n",
            "Cost after iteration 2660: 0.605977\n",
            "Cost after iteration 2670: 0.605620\n",
            "Cost after iteration 2680: 0.605264\n",
            "Cost after iteration 2690: 0.604908\n",
            "Cost after iteration 2700: 0.604553\n",
            "Cost after iteration 2710: 0.604199\n",
            "Cost after iteration 2720: 0.603846\n",
            "Cost after iteration 2730: 0.603493\n",
            "Cost after iteration 2740: 0.603141\n",
            "Cost after iteration 2750: 0.602791\n",
            "Cost after iteration 2760: 0.602440\n",
            "Cost after iteration 2770: 0.602091\n",
            "Cost after iteration 2780: 0.601743\n",
            "Cost after iteration 2790: 0.601395\n",
            "Cost after iteration 2800: 0.601048\n",
            "Cost after iteration 2810: 0.600702\n",
            "Cost after iteration 2820: 0.600356\n",
            "Cost after iteration 2830: 0.600012\n",
            "Cost after iteration 2840: 0.599668\n",
            "Cost after iteration 2850: 0.599325\n",
            "Cost after iteration 2860: 0.598982\n",
            "Cost after iteration 2870: 0.598641\n",
            "Cost after iteration 2880: 0.598300\n",
            "Cost after iteration 2890: 0.597960\n",
            "Cost after iteration 2900: 0.597620\n",
            "Cost after iteration 2910: 0.597281\n",
            "Cost after iteration 2920: 0.596943\n",
            "Cost after iteration 2930: 0.596606\n",
            "Cost after iteration 2940: 0.596270\n",
            "Cost after iteration 2950: 0.595934\n",
            "Cost after iteration 2960: 0.595598\n",
            "Cost after iteration 2970: 0.595264\n",
            "Cost after iteration 2980: 0.594930\n",
            "Cost after iteration 2990: 0.594597\n",
            "Cost after iteration 3000: 0.594265\n",
            "Cost after iteration 3010: 0.593933\n",
            "Cost after iteration 3020: 0.593602\n",
            "Cost after iteration 3030: 0.593272\n",
            "Cost after iteration 3040: 0.592942\n",
            "Cost after iteration 3050: 0.592613\n",
            "Cost after iteration 3060: 0.592285\n",
            "Cost after iteration 3070: 0.591957\n",
            "Cost after iteration 3080: 0.591630\n",
            "Cost after iteration 3090: 0.591304\n",
            "Cost after iteration 3100: 0.590978\n",
            "Cost after iteration 3110: 0.590653\n",
            "Cost after iteration 3120: 0.590329\n",
            "Cost after iteration 3130: 0.590005\n",
            "Cost after iteration 3140: 0.589682\n",
            "Cost after iteration 3150: 0.589359\n",
            "Cost after iteration 3160: 0.589038\n",
            "Cost after iteration 3170: 0.588716\n",
            "Cost after iteration 3180: 0.588396\n",
            "Cost after iteration 3190: 0.588076\n",
            "Cost after iteration 3200: 0.587757\n",
            "Cost after iteration 3210: 0.587438\n",
            "Cost after iteration 3220: 0.587120\n",
            "Cost after iteration 3230: 0.586802\n",
            "Cost after iteration 3240: 0.586485\n",
            "Cost after iteration 3250: 0.586169\n",
            "Cost after iteration 3260: 0.585854\n",
            "Cost after iteration 3270: 0.585538\n",
            "Cost after iteration 3280: 0.585224\n",
            "Cost after iteration 3290: 0.584910\n",
            "Cost after iteration 3300: 0.584597\n",
            "Cost after iteration 3310: 0.584284\n",
            "Cost after iteration 3320: 0.583972\n",
            "Cost after iteration 3330: 0.583660\n",
            "Cost after iteration 3340: 0.583349\n",
            "Cost after iteration 3350: 0.583039\n",
            "Cost after iteration 3360: 0.582729\n",
            "Cost after iteration 3370: 0.582420\n",
            "Cost after iteration 3380: 0.582111\n",
            "Cost after iteration 3390: 0.581803\n",
            "Cost after iteration 3400: 0.581496\n",
            "Cost after iteration 3410: 0.581188\n",
            "Cost after iteration 3420: 0.580882\n",
            "Cost after iteration 3430: 0.580576\n",
            "Cost after iteration 3440: 0.580271\n",
            "Cost after iteration 3450: 0.579966\n",
            "Cost after iteration 3460: 0.579662\n",
            "Cost after iteration 3470: 0.579358\n",
            "Cost after iteration 3480: 0.579055\n",
            "Cost after iteration 3490: 0.578752\n",
            "Cost after iteration 3500: 0.578450\n",
            "Cost after iteration 3510: 0.578148\n",
            "Cost after iteration 3520: 0.577847\n",
            "Cost after iteration 3530: 0.577547\n",
            "Cost after iteration 3540: 0.577247\n",
            "Cost after iteration 3550: 0.576947\n",
            "Cost after iteration 3560: 0.576648\n",
            "Cost after iteration 3570: 0.576350\n",
            "Cost after iteration 3580: 0.576052\n",
            "Cost after iteration 3590: 0.575755\n",
            "Cost after iteration 3600: 0.575458\n",
            "Cost after iteration 3610: 0.575161\n",
            "Cost after iteration 3620: 0.574865\n",
            "Cost after iteration 3630: 0.574570\n",
            "Cost after iteration 3640: 0.574275\n",
            "Cost after iteration 3650: 0.573981\n",
            "Cost after iteration 3660: 0.573687\n",
            "Cost after iteration 3670: 0.573393\n",
            "Cost after iteration 3680: 0.573100\n",
            "Cost after iteration 3690: 0.572808\n",
            "Cost after iteration 3700: 0.572516\n",
            "Cost after iteration 3710: 0.572225\n",
            "Cost after iteration 3720: 0.571934\n",
            "Cost after iteration 3730: 0.571643\n",
            "Cost after iteration 3740: 0.571353\n",
            "Cost after iteration 3750: 0.571064\n",
            "Cost after iteration 3760: 0.570775\n",
            "Cost after iteration 3770: 0.570486\n",
            "Cost after iteration 3780: 0.570198\n",
            "Cost after iteration 3790: 0.569910\n",
            "Cost after iteration 3800: 0.569623\n",
            "Cost after iteration 3810: 0.569336\n",
            "Cost after iteration 3820: 0.569050\n",
            "Cost after iteration 3830: 0.568764\n",
            "Cost after iteration 3840: 0.568479\n",
            "Cost after iteration 3850: 0.568194\n",
            "Cost after iteration 3860: 0.567909\n",
            "Cost after iteration 3870: 0.567625\n",
            "Cost after iteration 3880: 0.567342\n",
            "Cost after iteration 3890: 0.567059\n",
            "Cost after iteration 3900: 0.566776\n",
            "Cost after iteration 3910: 0.566494\n",
            "Cost after iteration 3920: 0.566212\n",
            "Cost after iteration 3930: 0.565931\n",
            "Cost after iteration 3940: 0.565650\n",
            "Cost after iteration 3950: 0.565369\n",
            "Cost after iteration 3960: 0.565089\n",
            "Cost after iteration 3970: 0.564810\n",
            "Cost after iteration 3980: 0.564531\n",
            "Cost after iteration 3990: 0.564252\n",
            "Cost after iteration 4000: 0.563974\n",
            "Cost after iteration 4010: 0.563696\n",
            "Cost after iteration 4020: 0.563418\n",
            "Cost after iteration 4030: 0.563141\n",
            "Cost after iteration 4040: 0.562865\n",
            "Cost after iteration 4050: 0.562589\n",
            "Cost after iteration 4060: 0.562313\n",
            "Cost after iteration 4070: 0.562037\n",
            "Cost after iteration 4080: 0.561762\n",
            "Cost after iteration 4090: 0.561488\n",
            "Cost after iteration 4100: 0.561214\n",
            "Cost after iteration 4110: 0.560940\n",
            "Cost after iteration 4120: 0.560667\n",
            "Cost after iteration 4130: 0.560394\n",
            "Cost after iteration 4140: 0.560121\n",
            "Cost after iteration 4150: 0.559849\n",
            "Cost after iteration 4160: 0.559578\n",
            "Cost after iteration 4170: 0.559306\n",
            "Cost after iteration 4180: 0.559035\n",
            "Cost after iteration 4190: 0.558765\n",
            "Cost after iteration 4200: 0.558495\n",
            "Cost after iteration 4210: 0.558225\n",
            "Cost after iteration 4220: 0.557956\n",
            "Cost after iteration 4230: 0.557687\n",
            "Cost after iteration 4240: 0.557418\n",
            "Cost after iteration 4250: 0.557150\n",
            "Cost after iteration 4260: 0.556882\n",
            "Cost after iteration 4270: 0.556615\n",
            "Cost after iteration 4280: 0.556348\n",
            "Cost after iteration 4290: 0.556081\n",
            "Cost after iteration 4300: 0.555815\n",
            "Cost after iteration 4310: 0.555549\n",
            "Cost after iteration 4320: 0.555284\n",
            "Cost after iteration 4330: 0.555019\n",
            "Cost after iteration 4340: 0.554754\n",
            "Cost after iteration 4350: 0.554489\n",
            "Cost after iteration 4360: 0.554225\n",
            "Cost after iteration 4370: 0.553962\n",
            "Cost after iteration 4380: 0.553698\n",
            "Cost after iteration 4390: 0.553436\n",
            "Cost after iteration 4400: 0.553173\n",
            "Cost after iteration 4410: 0.552911\n",
            "Cost after iteration 4420: 0.552649\n",
            "Cost after iteration 4430: 0.552388\n",
            "Cost after iteration 4440: 0.552127\n",
            "Cost after iteration 4450: 0.551866\n",
            "Cost after iteration 4460: 0.551605\n",
            "Cost after iteration 4470: 0.551345\n",
            "Cost after iteration 4480: 0.551086\n",
            "Cost after iteration 4490: 0.550826\n",
            "Cost after iteration 4500: 0.550567\n",
            "Cost after iteration 4510: 0.550309\n",
            "Cost after iteration 4520: 0.550051\n",
            "Cost after iteration 4530: 0.549793\n",
            "Cost after iteration 4540: 0.549535\n",
            "Cost after iteration 4550: 0.549278\n",
            "Cost after iteration 4560: 0.549021\n",
            "Cost after iteration 4570: 0.548764\n",
            "Cost after iteration 4580: 0.548508\n",
            "Cost after iteration 4590: 0.548252\n",
            "Cost after iteration 4600: 0.547997\n",
            "Cost after iteration 4610: 0.547742\n",
            "Cost after iteration 4620: 0.547487\n",
            "Cost after iteration 4630: 0.547232\n",
            "Cost after iteration 4640: 0.546978\n",
            "Cost after iteration 4650: 0.546724\n",
            "Cost after iteration 4660: 0.546471\n",
            "Cost after iteration 4670: 0.546218\n",
            "Cost after iteration 4680: 0.545965\n",
            "Cost after iteration 4690: 0.545712\n",
            "Cost after iteration 4700: 0.545460\n",
            "Cost after iteration 4710: 0.545208\n",
            "Cost after iteration 4720: 0.544957\n",
            "Cost after iteration 4730: 0.544706\n",
            "Cost after iteration 4740: 0.544455\n",
            "Cost after iteration 4750: 0.544204\n",
            "Cost after iteration 4760: 0.543954\n",
            "Cost after iteration 4770: 0.543704\n",
            "Cost after iteration 4780: 0.543455\n",
            "Cost after iteration 4790: 0.543205\n",
            "Cost after iteration 4800: 0.542956\n",
            "Cost after iteration 4810: 0.542708\n",
            "Cost after iteration 4820: 0.542459\n",
            "Cost after iteration 4830: 0.542211\n",
            "Cost after iteration 4840: 0.541964\n",
            "Cost after iteration 4850: 0.541716\n",
            "Cost after iteration 4860: 0.541469\n",
            "Cost after iteration 4870: 0.541222\n",
            "Cost after iteration 4880: 0.540976\n",
            "Cost after iteration 4890: 0.540730\n",
            "Cost after iteration 4900: 0.540484\n",
            "Cost after iteration 4910: 0.540238\n",
            "Cost after iteration 4920: 0.539993\n",
            "Cost after iteration 4930: 0.539748\n",
            "Cost after iteration 4940: 0.539504\n",
            "Cost after iteration 4950: 0.539259\n",
            "Cost after iteration 4960: 0.539015\n",
            "Cost after iteration 4970: 0.538772\n",
            "Cost after iteration 4980: 0.538528\n",
            "Cost after iteration 4990: 0.538285\n",
            "Cost after iteration 5000: 0.538042\n",
            "Cost after iteration 5010: 0.537800\n",
            "Cost after iteration 5020: 0.537557\n",
            "Cost after iteration 5030: 0.537315\n",
            "Cost after iteration 5040: 0.537074\n",
            "Cost after iteration 5050: 0.536832\n",
            "Cost after iteration 5060: 0.536591\n",
            "Cost after iteration 5070: 0.536351\n",
            "Cost after iteration 5080: 0.536110\n",
            "Cost after iteration 5090: 0.535870\n",
            "Cost after iteration 5100: 0.535630\n",
            "Cost after iteration 5110: 0.535390\n",
            "Cost after iteration 5120: 0.535151\n",
            "Cost after iteration 5130: 0.534912\n",
            "Cost after iteration 5140: 0.534673\n",
            "Cost after iteration 5150: 0.534435\n",
            "Cost after iteration 5160: 0.534197\n",
            "Cost after iteration 5170: 0.533959\n",
            "Cost after iteration 5180: 0.533721\n",
            "Cost after iteration 5190: 0.533484\n",
            "Cost after iteration 5200: 0.533247\n",
            "Cost after iteration 5210: 0.533010\n",
            "Cost after iteration 5220: 0.532773\n",
            "Cost after iteration 5230: 0.532537\n",
            "Cost after iteration 5240: 0.532301\n",
            "Cost after iteration 5250: 0.532065\n",
            "Cost after iteration 5260: 0.531830\n",
            "Cost after iteration 5270: 0.531595\n",
            "Cost after iteration 5280: 0.531360\n",
            "Cost after iteration 5290: 0.531125\n",
            "Cost after iteration 5300: 0.530891\n",
            "Cost after iteration 5310: 0.530657\n",
            "Cost after iteration 5320: 0.530423\n",
            "Cost after iteration 5330: 0.530190\n",
            "Cost after iteration 5340: 0.529956\n",
            "Cost after iteration 5350: 0.529723\n",
            "Cost after iteration 5360: 0.529491\n",
            "Cost after iteration 5370: 0.529258\n",
            "Cost after iteration 5380: 0.529026\n",
            "Cost after iteration 5390: 0.528794\n",
            "Cost after iteration 5400: 0.528563\n",
            "Cost after iteration 5410: 0.528331\n",
            "Cost after iteration 5420: 0.528100\n",
            "Cost after iteration 5430: 0.527869\n",
            "Cost after iteration 5440: 0.527639\n",
            "Cost after iteration 5450: 0.527408\n",
            "Cost after iteration 5460: 0.527178\n",
            "Cost after iteration 5470: 0.526948\n",
            "Cost after iteration 5480: 0.526719\n",
            "Cost after iteration 5490: 0.526489\n",
            "Cost after iteration 5500: 0.526260\n",
            "Cost after iteration 5510: 0.526032\n",
            "Cost after iteration 5520: 0.525803\n",
            "Cost after iteration 5530: 0.525575\n",
            "Cost after iteration 5540: 0.525347\n",
            "Cost after iteration 5550: 0.525119\n",
            "Cost after iteration 5560: 0.524891\n",
            "Cost after iteration 5570: 0.524664\n",
            "Cost after iteration 5580: 0.524437\n",
            "Cost after iteration 5590: 0.524210\n",
            "Cost after iteration 5600: 0.523984\n",
            "Cost after iteration 5610: 0.523757\n",
            "Cost after iteration 5620: 0.523531\n",
            "Cost after iteration 5630: 0.523306\n",
            "Cost after iteration 5640: 0.523080\n",
            "Cost after iteration 5650: 0.522855\n",
            "Cost after iteration 5660: 0.522630\n",
            "Cost after iteration 5670: 0.522405\n",
            "Cost after iteration 5680: 0.522180\n",
            "Cost after iteration 5690: 0.521956\n",
            "Cost after iteration 5700: 0.521732\n",
            "Cost after iteration 5710: 0.521508\n",
            "Cost after iteration 5720: 0.521285\n",
            "Cost after iteration 5730: 0.521061\n",
            "Cost after iteration 5740: 0.520838\n",
            "Cost after iteration 5750: 0.520615\n",
            "Cost after iteration 5760: 0.520393\n",
            "Cost after iteration 5770: 0.520170\n",
            "Cost after iteration 5780: 0.519948\n",
            "Cost after iteration 5790: 0.519726\n",
            "Cost after iteration 5800: 0.519504\n",
            "Cost after iteration 5810: 0.519283\n",
            "Cost after iteration 5820: 0.519062\n",
            "Cost after iteration 5830: 0.518841\n",
            "Cost after iteration 5840: 0.518620\n",
            "Cost after iteration 5850: 0.518400\n",
            "Cost after iteration 5860: 0.518179\n",
            "Cost after iteration 5870: 0.517959\n",
            "Cost after iteration 5880: 0.517739\n",
            "Cost after iteration 5890: 0.517520\n",
            "Cost after iteration 5900: 0.517300\n",
            "Cost after iteration 5910: 0.517081\n",
            "Cost after iteration 5920: 0.516862\n",
            "Cost after iteration 5930: 0.516644\n",
            "Cost after iteration 5940: 0.516425\n",
            "Cost after iteration 5950: 0.516207\n",
            "Cost after iteration 5960: 0.515989\n",
            "Cost after iteration 5970: 0.515771\n",
            "Cost after iteration 5980: 0.515554\n",
            "Cost after iteration 5990: 0.515337\n",
            "Cost after iteration 6000: 0.515120\n",
            "Cost after iteration 6010: 0.514903\n",
            "Cost after iteration 6020: 0.514686\n",
            "Cost after iteration 6030: 0.514470\n",
            "Cost after iteration 6040: 0.514254\n",
            "Cost after iteration 6050: 0.514038\n",
            "Cost after iteration 6060: 0.513822\n",
            "Cost after iteration 6070: 0.513606\n",
            "Cost after iteration 6080: 0.513391\n",
            "Cost after iteration 6090: 0.513176\n",
            "Cost after iteration 6100: 0.512961\n",
            "Cost after iteration 6110: 0.512747\n",
            "Cost after iteration 6120: 0.512532\n",
            "Cost after iteration 6130: 0.512318\n",
            "Cost after iteration 6140: 0.512104\n",
            "Cost after iteration 6150: 0.511890\n",
            "Cost after iteration 6160: 0.511677\n",
            "Cost after iteration 6170: 0.511463\n",
            "Cost after iteration 6180: 0.511250\n",
            "Cost after iteration 6190: 0.511037\n",
            "Cost after iteration 6200: 0.510825\n",
            "Cost after iteration 6210: 0.510612\n",
            "Cost after iteration 6220: 0.510400\n",
            "Cost after iteration 6230: 0.510188\n",
            "Cost after iteration 6240: 0.509976\n",
            "Cost after iteration 6250: 0.509764\n",
            "Cost after iteration 6260: 0.509553\n",
            "Cost after iteration 6270: 0.509342\n",
            "Cost after iteration 6280: 0.509131\n",
            "Cost after iteration 6290: 0.508920\n",
            "Cost after iteration 6300: 0.508710\n",
            "Cost after iteration 6310: 0.508499\n",
            "Cost after iteration 6320: 0.508289\n",
            "Cost after iteration 6330: 0.508079\n",
            "Cost after iteration 6340: 0.507869\n",
            "Cost after iteration 6350: 0.507660\n",
            "Cost after iteration 6360: 0.507451\n",
            "Cost after iteration 6370: 0.507241\n",
            "Cost after iteration 6380: 0.507033\n",
            "Cost after iteration 6390: 0.506824\n",
            "Cost after iteration 6400: 0.506615\n",
            "Cost after iteration 6410: 0.506407\n",
            "Cost after iteration 6420: 0.506199\n",
            "Cost after iteration 6430: 0.505991\n",
            "Cost after iteration 6440: 0.505783\n",
            "Cost after iteration 6450: 0.505576\n",
            "Cost after iteration 6460: 0.505369\n",
            "Cost after iteration 6470: 0.505162\n",
            "Cost after iteration 6480: 0.504955\n",
            "Cost after iteration 6490: 0.504748\n",
            "Cost after iteration 6500: 0.504542\n",
            "Cost after iteration 6510: 0.504335\n",
            "Cost after iteration 6520: 0.504129\n",
            "Cost after iteration 6530: 0.503924\n",
            "Cost after iteration 6540: 0.503718\n",
            "Cost after iteration 6550: 0.503512\n",
            "Cost after iteration 6560: 0.503307\n",
            "Cost after iteration 6570: 0.503102\n",
            "Cost after iteration 6580: 0.502897\n",
            "Cost after iteration 6590: 0.502693\n",
            "Cost after iteration 6600: 0.502488\n",
            "Cost after iteration 6610: 0.502284\n",
            "Cost after iteration 6620: 0.502080\n",
            "Cost after iteration 6630: 0.501876\n",
            "Cost after iteration 6640: 0.501672\n",
            "Cost after iteration 6650: 0.501469\n",
            "Cost after iteration 6660: 0.501265\n",
            "Cost after iteration 6670: 0.501062\n",
            "Cost after iteration 6680: 0.500859\n",
            "Cost after iteration 6690: 0.500657\n",
            "Cost after iteration 6700: 0.500454\n",
            "Cost after iteration 6710: 0.500252\n",
            "Cost after iteration 6720: 0.500050\n",
            "Cost after iteration 6730: 0.499848\n",
            "Cost after iteration 6740: 0.499646\n",
            "Cost after iteration 6750: 0.499444\n",
            "Cost after iteration 6760: 0.499243\n",
            "Cost after iteration 6770: 0.499042\n",
            "Cost after iteration 6780: 0.498841\n",
            "Cost after iteration 6790: 0.498640\n",
            "Cost after iteration 6800: 0.498439\n",
            "Cost after iteration 6810: 0.498239\n",
            "Cost after iteration 6820: 0.498038\n",
            "Cost after iteration 6830: 0.497838\n",
            "Cost after iteration 6840: 0.497638\n",
            "Cost after iteration 6850: 0.497439\n",
            "Cost after iteration 6860: 0.497239\n",
            "Cost after iteration 6870: 0.497040\n",
            "Cost after iteration 6880: 0.496841\n",
            "Cost after iteration 6890: 0.496642\n",
            "Cost after iteration 6900: 0.496443\n",
            "Cost after iteration 6910: 0.496245\n",
            "Cost after iteration 6920: 0.496046\n",
            "Cost after iteration 6930: 0.495848\n",
            "Cost after iteration 6940: 0.495650\n",
            "Cost after iteration 6950: 0.495452\n",
            "Cost after iteration 6960: 0.495254\n",
            "Cost after iteration 6970: 0.495057\n",
            "Cost after iteration 6980: 0.494860\n",
            "Cost after iteration 6990: 0.494662\n",
            "Cost after iteration 7000: 0.494466\n",
            "Cost after iteration 7010: 0.494269\n",
            "Cost after iteration 7020: 0.494072\n",
            "Cost after iteration 7030: 0.493876\n",
            "Cost after iteration 7040: 0.493680\n",
            "Cost after iteration 7050: 0.493484\n",
            "Cost after iteration 7060: 0.493288\n",
            "Cost after iteration 7070: 0.493092\n",
            "Cost after iteration 7080: 0.492896\n",
            "Cost after iteration 7090: 0.492701\n",
            "Cost after iteration 7100: 0.492506\n",
            "Cost after iteration 7110: 0.492311\n",
            "Cost after iteration 7120: 0.492116\n",
            "Cost after iteration 7130: 0.491922\n",
            "Cost after iteration 7140: 0.491727\n",
            "Cost after iteration 7150: 0.491533\n",
            "Cost after iteration 7160: 0.491339\n",
            "Cost after iteration 7170: 0.491145\n",
            "Cost after iteration 7180: 0.490951\n",
            "Cost after iteration 7190: 0.490758\n",
            "Cost after iteration 7200: 0.490564\n",
            "Cost after iteration 7210: 0.490371\n",
            "Cost after iteration 7220: 0.490178\n",
            "Cost after iteration 7230: 0.489985\n",
            "Cost after iteration 7240: 0.489792\n",
            "Cost after iteration 7250: 0.489600\n",
            "Cost after iteration 7260: 0.489408\n",
            "Cost after iteration 7270: 0.489215\n",
            "Cost after iteration 7280: 0.489023\n",
            "Cost after iteration 7290: 0.488832\n",
            "Cost after iteration 7300: 0.488640\n",
            "Cost after iteration 7310: 0.488448\n",
            "Cost after iteration 7320: 0.488257\n",
            "Cost after iteration 7330: 0.488066\n",
            "Cost after iteration 7340: 0.487875\n",
            "Cost after iteration 7350: 0.487684\n",
            "Cost after iteration 7360: 0.487493\n",
            "Cost after iteration 7370: 0.487303\n",
            "Cost after iteration 7380: 0.487113\n",
            "Cost after iteration 7390: 0.486923\n",
            "Cost after iteration 7400: 0.486733\n",
            "Cost after iteration 7410: 0.486543\n",
            "Cost after iteration 7420: 0.486353\n",
            "Cost after iteration 7430: 0.486164\n",
            "Cost after iteration 7440: 0.485974\n",
            "Cost after iteration 7450: 0.485785\n",
            "Cost after iteration 7460: 0.485596\n",
            "Cost after iteration 7470: 0.485407\n",
            "Cost after iteration 7480: 0.485219\n",
            "Cost after iteration 7490: 0.485030\n",
            "Cost after iteration 7500: 0.484842\n",
            "Cost after iteration 7510: 0.484654\n",
            "Cost after iteration 7520: 0.484466\n",
            "Cost after iteration 7530: 0.484278\n",
            "Cost after iteration 7540: 0.484091\n",
            "Cost after iteration 7550: 0.483903\n",
            "Cost after iteration 7560: 0.483716\n",
            "Cost after iteration 7570: 0.483529\n",
            "Cost after iteration 7580: 0.483342\n",
            "Cost after iteration 7590: 0.483155\n",
            "Cost after iteration 7600: 0.482968\n",
            "Cost after iteration 7610: 0.482782\n",
            "Cost after iteration 7620: 0.482595\n",
            "Cost after iteration 7630: 0.482409\n",
            "Cost after iteration 7640: 0.482223\n",
            "Cost after iteration 7650: 0.482037\n",
            "Cost after iteration 7660: 0.481851\n",
            "Cost after iteration 7670: 0.481666\n",
            "Cost after iteration 7680: 0.481481\n",
            "Cost after iteration 7690: 0.481295\n",
            "Cost after iteration 7700: 0.481110\n",
            "Cost after iteration 7710: 0.480925\n",
            "Cost after iteration 7720: 0.480741\n",
            "Cost after iteration 7730: 0.480556\n",
            "Cost after iteration 7740: 0.480372\n",
            "Cost after iteration 7750: 0.480187\n",
            "Cost after iteration 7760: 0.480003\n",
            "Cost after iteration 7770: 0.479819\n",
            "Cost after iteration 7780: 0.479636\n",
            "Cost after iteration 7790: 0.479452\n",
            "Cost after iteration 7800: 0.479268\n",
            "Cost after iteration 7810: 0.479085\n",
            "Cost after iteration 7820: 0.478902\n",
            "Cost after iteration 7830: 0.478719\n",
            "Cost after iteration 7840: 0.478536\n",
            "Cost after iteration 7850: 0.478353\n",
            "Cost after iteration 7860: 0.478171\n",
            "Cost after iteration 7870: 0.477988\n",
            "Cost after iteration 7880: 0.477806\n",
            "Cost after iteration 7890: 0.477624\n",
            "Cost after iteration 7900: 0.477442\n",
            "Cost after iteration 7910: 0.477260\n",
            "Cost after iteration 7920: 0.477079\n",
            "Cost after iteration 7930: 0.476897\n",
            "Cost after iteration 7940: 0.476716\n",
            "Cost after iteration 7950: 0.476535\n",
            "Cost after iteration 7960: 0.476354\n",
            "Cost after iteration 7970: 0.476173\n",
            "Cost after iteration 7980: 0.475992\n",
            "Cost after iteration 7990: 0.475812\n",
            "Cost after iteration 8000: 0.475631\n",
            "Cost after iteration 8010: 0.475451\n",
            "Cost after iteration 8020: 0.475271\n",
            "Cost after iteration 8030: 0.475091\n",
            "Cost after iteration 8040: 0.474911\n",
            "Cost after iteration 8050: 0.474732\n",
            "Cost after iteration 8060: 0.474552\n",
            "Cost after iteration 8070: 0.474373\n",
            "Cost after iteration 8080: 0.474194\n",
            "Cost after iteration 8090: 0.474015\n",
            "Cost after iteration 8100: 0.473836\n",
            "Cost after iteration 8110: 0.473657\n",
            "Cost after iteration 8120: 0.473478\n",
            "Cost after iteration 8130: 0.473300\n",
            "Cost after iteration 8140: 0.473122\n",
            "Cost after iteration 8150: 0.472944\n",
            "Cost after iteration 8160: 0.472766\n",
            "Cost after iteration 8170: 0.472588\n",
            "Cost after iteration 8180: 0.472410\n",
            "Cost after iteration 8190: 0.472232\n",
            "Cost after iteration 8200: 0.472055\n",
            "Cost after iteration 8210: 0.471878\n",
            "Cost after iteration 8220: 0.471701\n",
            "Cost after iteration 8230: 0.471524\n",
            "Cost after iteration 8240: 0.471347\n",
            "Cost after iteration 8250: 0.471170\n",
            "Cost after iteration 8260: 0.470994\n",
            "Cost after iteration 8270: 0.470817\n",
            "Cost after iteration 8280: 0.470641\n",
            "Cost after iteration 8290: 0.470465\n",
            "Cost after iteration 8300: 0.470289\n",
            "Cost after iteration 8310: 0.470113\n",
            "Cost after iteration 8320: 0.469938\n",
            "Cost after iteration 8330: 0.469762\n",
            "Cost after iteration 8340: 0.469587\n",
            "Cost after iteration 8350: 0.469411\n",
            "Cost after iteration 8360: 0.469236\n",
            "Cost after iteration 8370: 0.469061\n",
            "Cost after iteration 8380: 0.468887\n",
            "Cost after iteration 8390: 0.468712\n",
            "Cost after iteration 8400: 0.468537\n",
            "Cost after iteration 8410: 0.468363\n",
            "Cost after iteration 8420: 0.468189\n",
            "Cost after iteration 8430: 0.468015\n",
            "Cost after iteration 8440: 0.467841\n",
            "Cost after iteration 8450: 0.467667\n",
            "Cost after iteration 8460: 0.467493\n",
            "Cost after iteration 8470: 0.467320\n",
            "Cost after iteration 8480: 0.467146\n",
            "Cost after iteration 8490: 0.466973\n",
            "Cost after iteration 8500: 0.466800\n",
            "Cost after iteration 8510: 0.466627\n",
            "Cost after iteration 8520: 0.466454\n",
            "Cost after iteration 8530: 0.466282\n",
            "Cost after iteration 8540: 0.466109\n",
            "Cost after iteration 8550: 0.465937\n",
            "Cost after iteration 8560: 0.465764\n",
            "Cost after iteration 8570: 0.465592\n",
            "Cost after iteration 8580: 0.465420\n",
            "Cost after iteration 8590: 0.465248\n",
            "Cost after iteration 8600: 0.465077\n",
            "Cost after iteration 8610: 0.464905\n",
            "Cost after iteration 8620: 0.464734\n",
            "Cost after iteration 8630: 0.464562\n",
            "Cost after iteration 8640: 0.464391\n",
            "Cost after iteration 8650: 0.464220\n",
            "Cost after iteration 8660: 0.464049\n",
            "Cost after iteration 8670: 0.463879\n",
            "Cost after iteration 8680: 0.463708\n",
            "Cost after iteration 8690: 0.463537\n",
            "Cost after iteration 8700: 0.463367\n",
            "Cost after iteration 8710: 0.463197\n",
            "Cost after iteration 8720: 0.463027\n",
            "Cost after iteration 8730: 0.462857\n",
            "Cost after iteration 8740: 0.462687\n",
            "Cost after iteration 8750: 0.462517\n",
            "Cost after iteration 8760: 0.462348\n",
            "Cost after iteration 8770: 0.462179\n",
            "Cost after iteration 8780: 0.462009\n",
            "Cost after iteration 8790: 0.461840\n",
            "Cost after iteration 8800: 0.461671\n",
            "Cost after iteration 8810: 0.461502\n",
            "Cost after iteration 8820: 0.461334\n",
            "Cost after iteration 8830: 0.461165\n",
            "Cost after iteration 8840: 0.460997\n",
            "Cost after iteration 8850: 0.460828\n",
            "Cost after iteration 8860: 0.460660\n",
            "Cost after iteration 8870: 0.460492\n",
            "Cost after iteration 8880: 0.460324\n",
            "Cost after iteration 8890: 0.460156\n",
            "Cost after iteration 8900: 0.459989\n",
            "Cost after iteration 8910: 0.459821\n",
            "Cost after iteration 8920: 0.459654\n",
            "Cost after iteration 8930: 0.459487\n",
            "Cost after iteration 8940: 0.459319\n",
            "Cost after iteration 8950: 0.459152\n",
            "Cost after iteration 8960: 0.458986\n",
            "Cost after iteration 8970: 0.458819\n",
            "Cost after iteration 8980: 0.458652\n",
            "Cost after iteration 8990: 0.458486\n",
            "Cost after iteration 9000: 0.458319\n",
            "Cost after iteration 9010: 0.458153\n",
            "Cost after iteration 9020: 0.457987\n",
            "Cost after iteration 9030: 0.457821\n",
            "Cost after iteration 9040: 0.457655\n",
            "Cost after iteration 9050: 0.457490\n",
            "Cost after iteration 9060: 0.457324\n",
            "Cost after iteration 9070: 0.457159\n",
            "Cost after iteration 9080: 0.456993\n",
            "Cost after iteration 9090: 0.456828\n",
            "Cost after iteration 9100: 0.456663\n",
            "Cost after iteration 9110: 0.456498\n",
            "Cost after iteration 9120: 0.456333\n",
            "Cost after iteration 9130: 0.456169\n",
            "Cost after iteration 9140: 0.456004\n",
            "Cost after iteration 9150: 0.455840\n",
            "Cost after iteration 9160: 0.455675\n",
            "Cost after iteration 9170: 0.455511\n",
            "Cost after iteration 9180: 0.455347\n",
            "Cost after iteration 9190: 0.455183\n",
            "Cost after iteration 9200: 0.455020\n",
            "Cost after iteration 9210: 0.454856\n",
            "Cost after iteration 9220: 0.454692\n",
            "Cost after iteration 9230: 0.454529\n",
            "Cost after iteration 9240: 0.454366\n",
            "Cost after iteration 9250: 0.454203\n",
            "Cost after iteration 9260: 0.454040\n",
            "Cost after iteration 9270: 0.453877\n",
            "Cost after iteration 9280: 0.453714\n",
            "Cost after iteration 9290: 0.453551\n",
            "Cost after iteration 9300: 0.453389\n",
            "Cost after iteration 9310: 0.453226\n",
            "Cost after iteration 9320: 0.453064\n",
            "Cost after iteration 9330: 0.452902\n",
            "Cost after iteration 9340: 0.452740\n",
            "Cost after iteration 9350: 0.452578\n",
            "Cost after iteration 9360: 0.452416\n",
            "Cost after iteration 9370: 0.452255\n",
            "Cost after iteration 9380: 0.452093\n",
            "Cost after iteration 9390: 0.451932\n",
            "Cost after iteration 9400: 0.451770\n",
            "Cost after iteration 9410: 0.451609\n",
            "Cost after iteration 9420: 0.451448\n",
            "Cost after iteration 9430: 0.451287\n",
            "Cost after iteration 9440: 0.451127\n",
            "Cost after iteration 9450: 0.450966\n",
            "Cost after iteration 9460: 0.450805\n",
            "Cost after iteration 9470: 0.450645\n",
            "Cost after iteration 9480: 0.450485\n",
            "Cost after iteration 9490: 0.450324\n",
            "Cost after iteration 9500: 0.450164\n",
            "Cost after iteration 9510: 0.450004\n",
            "Cost after iteration 9520: 0.449845\n",
            "Cost after iteration 9530: 0.449685\n",
            "Cost after iteration 9540: 0.449525\n",
            "Cost after iteration 9550: 0.449366\n",
            "Cost after iteration 9560: 0.449207\n",
            "Cost after iteration 9570: 0.449047\n",
            "Cost after iteration 9580: 0.448888\n",
            "Cost after iteration 9590: 0.448729\n",
            "Cost after iteration 9600: 0.448570\n",
            "Cost after iteration 9610: 0.448412\n",
            "Cost after iteration 9620: 0.448253\n",
            "Cost after iteration 9630: 0.448095\n",
            "Cost after iteration 9640: 0.447936\n",
            "Cost after iteration 9650: 0.447778\n",
            "Cost after iteration 9660: 0.447620\n",
            "Cost after iteration 9670: 0.447462\n",
            "Cost after iteration 9680: 0.447304\n",
            "Cost after iteration 9690: 0.447146\n",
            "Cost after iteration 9700: 0.446989\n",
            "Cost after iteration 9710: 0.446831\n",
            "Cost after iteration 9720: 0.446674\n",
            "Cost after iteration 9730: 0.446516\n",
            "Cost after iteration 9740: 0.446359\n",
            "Cost after iteration 9750: 0.446202\n",
            "Cost after iteration 9760: 0.446045\n",
            "Cost after iteration 9770: 0.445888\n",
            "Cost after iteration 9780: 0.445731\n",
            "Cost after iteration 9790: 0.445575\n",
            "Cost after iteration 9800: 0.445418\n",
            "Cost after iteration 9810: 0.445262\n",
            "Cost after iteration 9820: 0.445106\n",
            "Cost after iteration 9830: 0.444950\n",
            "Cost after iteration 9840: 0.444794\n",
            "Cost after iteration 9850: 0.444638\n",
            "Cost after iteration 9860: 0.444482\n",
            "Cost after iteration 9870: 0.444326\n",
            "Cost after iteration 9880: 0.444171\n",
            "Cost after iteration 9890: 0.444015\n",
            "Cost after iteration 9900: 0.443860\n",
            "Cost after iteration 9910: 0.443705\n",
            "Cost after iteration 9920: 0.443550\n",
            "Cost after iteration 9930: 0.443395\n",
            "Cost after iteration 9940: 0.443240\n",
            "Cost after iteration 9950: 0.443085\n",
            "Cost after iteration 9960: 0.442930\n",
            "Cost after iteration 9970: 0.442776\n",
            "Cost after iteration 9980: 0.442621\n",
            "Cost after iteration 9990: 0.442467\n",
            "train accuracy: 81.80844735276621 %\n",
            "test accuracy: 70.74829931972789 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0ugkgwa6uRo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Your Assignment\n",
        "In cells below (or in a Python script), include the following:\n",
        "\n",
        "*   Try to improve the prediction.  To do this, you can modify the training and testing sets (perhaps we need more non-cats in training).  You can try a different activation function.  You can obviously try different numbers of iterations.  Be creative.  Report everything you have tried.  For each attempt include code, output, and a discussion.\n",
        "*   Derive the derivatives of the cost function, `dw` and `db`.\n",
        "*  In a couple of paragraphs, summarize what you have learned.  Discuss limitations of the approach.  Image recognition is not done this way in production environments!  Discuss how it is done by data scientists today.  Google around.  Some of you will be using these methods for your FCC.\n",
        "\n",
        "\n"
      ]
    }
  ]
}