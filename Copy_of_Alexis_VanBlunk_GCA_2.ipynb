{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Alexis VanBlunk - GCA_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgvb1899/GCA-2/blob/Development/Copy_of_Alexis_VanBlunk_GCA_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LefCCwDeB3Nm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "XKoEENrWUX5j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GCA 2 will require approximately four hours of your time.  You may complete GCA 2 in Colab or locally on your machines.  Please post any code you develop to GitHub.  Make sure that each of your group members has access to your group repository.\n",
        "\n",
        "In GCA 2, you will code your own model to identify images of cats from other images.  Specifically, you will code a logistic regression classisier.  This assignment was inspired by Andrew Ng's Deep Learning course offered by deeplearning.ai, adapted from your instructor's own notes.  Your instructor assembled the data.  Images were taken from the Cats and Dogs dataset on Kaggle and Caltech 101 Image Dataset.\n",
        "\n",
        "Let's get started."
      ]
    },
    {
      "metadata": {
        "id": "K1eiqQnEB8_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1: Mount Google Drive and Download Images"
      ]
    },
    {
      "metadata": {
        "id": "FsisUbEuc186",
        "colab_type": "code",
        "outputId": "cd962457-9df2-4667-8d14-ad9394b64e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kkT5AegfWN78",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download images from the following link:\n",
        "\n",
        "https://drive.google.com/open?id=1pNbSKJNl4SUVguMpIfK7UI9CtyubbS9M\n",
        "\n",
        "Place the file contaning the images (`images.h5`) in a convenient place in your Google Drive."
      ]
    },
    {
      "metadata": {
        "id": "AvGYVtveCCmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import Modules"
      ]
    },
    {
      "metadata": {
        "id": "Cmsb2emlYBmr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import the modules `h5py`, `numpy`, and `matplotlib`."
      ]
    },
    {
      "metadata": {
        "id": "kq39u35J-2Iy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z4VWXNAyCKtn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3:  Read Data File"
      ]
    },
    {
      "metadata": {
        "id": "Jd6aGOgXYQJ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read the HDF5 file containing the images.  An HDF5 file is a smart data container.  For our purposes now, the file contains multi-dimensional arrays related to our images.\n",
        "\n",
        "`Train/images`\n",
        "<br>\n",
        "Contains 4405 training images each with resolution 100 x 100.  It is a 4D NumPy array with shape `(num_images,100,100,3)`.  We read this data into the variable `images_train` below.\n",
        "\n",
        "`Test/images`\n",
        "<br>\n",
        "Contains 1617 testing images each with resolution 100 x 100.  It is a 4D NumPy array with shape `(num_images,100,100,3)`.  We read this data into the variable `images_test` below.\n",
        "\n",
        "`Train/labels`\n",
        "<br>\n",
        "A one-dimensional array with length equal to the number of training images.  It contains 1 or 0.  1 for cat.  0 for not cat.  The row in this array corresponds to the first index of `images_train`.  We read this data into the variable `labels_train` below.\n",
        "\n",
        "`Test/labels`\n",
        "<br>\n",
        "A one-dimensional array with length equal to the number of testing images.  It contains 1 or 0.  1 for cat.  0 for not cat.  The row in this array corresponds to the first index of `images_test`.  We read this data into the variable `labels_test` below."
      ]
    },
    {
      "metadata": {
        "id": "mMQXqZgLQQly",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from h5py import*\n",
        "f = File('/content/drive/My Drive/images_no_cats.h5', 'r')\n",
        "images_new = f['images'][...]\n",
        "images = []\n",
        "for i in range(4000):\n",
        "  images = images + [images_new[i]]\n",
        "labels = f['labels'][...]\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m0dsj7a4-WMw",
        "colab_type": "code",
        "outputId": "3822a3ba-ea1f-4520-9479-e4b57bffc3ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "f = h5py.File('/content/drive/My Drive/images.h5', 'r')\n",
        "\n",
        "images_train = f['Train/images'][...]\n",
        "images_train = np.concatenate((images_train,images), axis = 0)\n",
        "labels_train = f['Train/labels'][...]\n",
        "zeros = np.zeros(4000)\n",
        "labels_train = np.concatenate((labels_train, zeros))\n",
        "\n",
        "images_test = f['Test/images'][...]\n",
        "labels_test = f['Test/labels'][...]\n",
        "\n",
        "f.close()\n",
        "print(np.shape(images_train))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8405, 100, 100, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qSmVGQLKCSLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4: Plot Some Data"
      ]
    },
    {
      "metadata": {
        "id": "3ctpBPKjb84a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's get a better understanding of what these arrays contain.  Let's plot a cat and its correpsonding label."
      ]
    },
    {
      "metadata": {
        "id": "gb0-noeEcFsp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = images_train[1000]\n",
        "label = labels_train[1000]\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "print('The label of this image is ', label, '.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGjLIm7vcj7Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look of an image without a cat."
      ]
    },
    {
      "metadata": {
        "id": "Co59Pan1cm7S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = images_train[3]\n",
        "label = labels_train[3]\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "print('The label of this image is ', label, '.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPplDIBWc9C0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice how the stapler has label 0.  This is because it is not a cat.  Only images with cats are labeled 1.  Also, notice that the images have been re-sized to have the same resolution.  This is necessary as our model will only accept RGB images of resolution 100 x 100.\n",
        "\n",
        "What is `image`?  `image` is an image, but in Python, its simply a NumPy array.  It's a 100 x 100 x 3 NumPy array.  100 x 100 is the image resolution.  The `3` indicates that the image in encoded in RGB.  `[:,:,0]` is the red channel.  `[:,:,1]` is the green channel.  Finally, `[:,:,2]` is the blue channel.\n",
        "\n",
        "Here is a better indication of the types of images in our dataset."
      ]
    },
    {
      "metadata": {
        "id": "-dFmNPWBBaEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(500,600):\n",
        "    plt.subplot(10, 10, i-499)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(images_test[i])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_neQGQICX98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 5: Flatten the Images"
      ]
    },
    {
      "metadata": {
        "id": "RJkg_uZ_eeyM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For our model, we'll need to flatten our images.  Remember, each image is `100 x 100 x 3`.  We will preserve each pixel, but require a different pixel arrangement.  We convert each three-dimension NumPy image array to a one-dimension array using the method `flatten`."
      ]
    },
    {
      "metadata": {
        "id": "o9zSKouz_knw",
        "colab_type": "code",
        "outputId": "ee9e2a0a-8813-4864-ee4b-1287a0871260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "num_train = images_train.shape[0]\n",
        "images_train_flatten = images_train.flatten().reshape(num_train, 100*100*3)\n",
        "print(\"Number Training Images: \", num_train)\n",
        "print(\"Shape of Flattened Training Images Array: \", images_train_flatten.shape)\n",
        "\n",
        "num_test = images_test.shape[0]\n",
        "images_test_flatten = images_test.flatten().reshape(num_test, 100*100*3)\n",
        "print(\"Number Testing Images: \", num_test)\n",
        "print(\"Shape of Flattened Testing Images Array: \", images_test_flatten.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number Training Images:  8405\n",
            "Shape of Flattened Training Images Array:  (8405, 30000)\n",
            "Number Testing Images:  1617\n",
            "Shape of Flattened Testing Images Array:  (1617, 30000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vce-ZwhBDeqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 6: Standardize the Data"
      ]
    },
    {
      "metadata": {
        "id": "tenS_NfBfV2j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Standardizing data is a very important step when creating machine leanring models.  What happens if our photos of cats were taken at different exposures?  We'd have to compensate for this, otherwise we'd might \"train\" our model to discriminate different exposures when classifying cats.  This is a waste and could compromise the accuracy of our model!  Instead, we normalize our images based on the highest permissible pixel value, 255."
      ]
    },
    {
      "metadata": {
        "id": "xQ_2SRSIDck9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_set_x = images_train_flatten/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3F0O21WNvaT-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_set_x = images_test_flatten/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zep1JnGiE37p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 7: Assembling the Model"
      ]
    },
    {
      "metadata": {
        "id": "NGcQ_r2igdcb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following image is from Andrew Ng's course on Deep Learning.  What are we looking at?  Basically, this is our predictor.  It accepts three imputs: a scalar bias, an array of weights with length equal to the total number of pixels in a given image (3x100x100), and flattened image(s).  You will learn how to train this predictor for improved identification.  Effectively, your goal is to settle on an optimized scalar bias and array of weights."
      ]
    },
    {
      "metadata": {
        "id": "7k1d9H8LgXDz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/andersy005/deep-learning-specialization-coursera/raw/8fd8ad74a9b2d8321bb0a701c5c0f3ec4abad144/01-Neural-Networks-and-Deep-Learning/week2/Programming-Assignments/images/LogReg_kiank.png)"
      ]
    },
    {
      "metadata": {
        "id": "EXOompHimx2T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Working from left to right, an image is flattened and normalized (we've already done this).  Each pixel is multiplied by its corresponding weight.  The products are summed.  The scalar bias is then added to this summation.  This result, in turn, is pushed through the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function).  The sigmoid function is known as an activation function and outputs a scalar between 0 and 1.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*Xu7B5y9gp0iL5ooBj7LtWw.png)\n",
        "\n",
        "Once our model is sufficiently trained, you can think of this scalar as a probability that the image contains a cat.  For example, if the \"probability\" is greater than 0.5, we classify the image as cat."
      ]
    },
    {
      "metadata": {
        "id": "arVQzQnboikO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start coding pieces of the model."
      ]
    },
    {
      "metadata": {
        "id": "j7Or4nBFE_Yv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sigmoid Function"
      ]
    },
    {
      "metadata": {
        "id": "e05cW1Q_FCf6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  \n",
        "    \"\"\"\n",
        "    Computes the sigmoid function.\n",
        "    \n",
        "    input:  scalar or numpy array\n",
        "    output: scalar or numpy array\n",
        "    \"\"\"\n",
        "\n",
        "    s = 1. / ( 1 + np.exp(-z))\n",
        "    \n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LsY7_zb2FekF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Weights and Biases\n",
        "To initialize the model, we need to zero-out our weight array and scalar bias.  Recall that each pixel is assigned its own weight."
      ]
    },
    {
      "metadata": {
        "id": "fLHbLozyFhet",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_model(dim):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns initialized weight array and bias scalar.\n",
        "    \n",
        "    input:  number of weights\n",
        "    output: weights (array), biases (scalar)\n",
        "    \"\"\"\n",
        "    \n",
        "    w = np.zeros(shape=(dim, 1), dtype=float)\n",
        "    b = 0\n",
        "    \n",
        "    return w,b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oKeDB96cGgLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 8: Training the Model\n",
        "Training the model consists of iteratively determining an optimized set of input parameters.  These parameters are the weights and scalar bias.\n",
        "\n",
        "We first define forward propagation.  Forward propagation consists of exercising the weights and bias on all available training images, calculating an activation from the sigmoid function for each, and finally calculating a cost (or loss) associated with our \"predictions\".  We use the cross-entropy cost.  Please read [here](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#cost-function) for more details.  Regardless of number of images, cost will always be a scalar.  The higher the cost, the worse our predictions."
      ]
    },
    {
      "metadata": {
        "id": "lbnh3YA3Gqyw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_propagate(X, Y, w, b):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns array of activations (one per image)\n",
        "    and cost (scalar).\n",
        "    \n",
        "    input:  X     array of flattened images (num_pixels,num_images)\n",
        "            Y     array of labels, 1 or 0 (num_images)\n",
        "            w     array of weights (num_pixels,1)\n",
        "            b     scalar bias (float)\n",
        "    output: A     array of activations (num_images)\n",
        "            cost  loss (float)\n",
        "    \"\"\"\n",
        "  \n",
        "    # get number of images under consideration\n",
        "    num_images = X.shape[1]\n",
        "  \n",
        "    # calculate the activation for each image\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "  \n",
        "    # calculate the cost (scalar) using cross-entropy\n",
        "    cost = np.squeeze( (-1. / num_images) * np.sum((Y*np.log(A)+(1-Y)*np.log(1-A)),axis=1) )\n",
        "  \n",
        "    return A, cost  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VMvMovQYv7Qs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We propagate backwards to minimize our cost.  This involves taking derivatives of the cost function.  We take derivatives in terms of our weights and scalar bias.  For more info, read [here](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#gradient-descent).  Without this function, our optimization routine is aimless."
      ]
    },
    {
      "metadata": {
        "id": "-wamExdGKMpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backward_propagate(X, Y, A):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns a dictionary of derivatives of cost function.\n",
        "    \n",
        "    input:  X      array of flattened images (num_pixels,num_images)\n",
        "            Y      array of labels, 1 or 0 (num_images)\n",
        "            A      array of activations (num_images)\n",
        "    output: grads  dictionary with keys dw and db\n",
        "    \"\"\"\n",
        "    \n",
        "    # get number of images under consideration\n",
        "    num_images = X.shape[1]\n",
        "  \n",
        "    # derivative of cost function wrt w (scalar)\n",
        "    dw = (1./num_images)*np.dot(X,((A-Y).T))\n",
        "    \n",
        "    # derivative of cost function wrt b (scalar)\n",
        "    db = (1./num_images)*np.sum(A-Y,axis=1)\n",
        "  \n",
        "    # create dictionary of derivatives (gradients)\n",
        "    grads = {\"dw\": dw, \"db\": db}\n",
        "  \n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8YRAhYI50Azb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now assemble our optimization routine.  This routine is known as gradient descent.  It is an iterative process.  For each iteration, we conduct forward and backward propagation.  We update our array of weights and scalar bias by subtracting gradients from their previous values.  We apply a learning rate to this update.  If the learning rate is too high, our optimization procedure will not converge.  If the learning rate is too low, our optimization routine will converge to a solution, albeit slowly.  "
      ]
    },
    {
      "metadata": {
        "id": "vDSM-UzMLZyY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \n",
        "    \"\"\"\n",
        "    Optimize array of weights and scalar bias.\n",
        "    \n",
        "    input:  w               array of weights (num_pixels,1)\n",
        "            b               scalar bias (float)\n",
        "            X               array of flattened images (num_pixels,num_images)\n",
        "            Y               array of labels, 1 or 0 (num_images)\n",
        "            num_iterations  number of iterations for optimization (scalar)\n",
        "            learning_rate   gradient multiplier (scalar)\n",
        "            print_cost      boolean controlling user feedback\n",
        "    output: params          w and b after num_iterations of optimization\n",
        "            grads           dictionary with keys dw and db\n",
        "            costs           history of cost during optimization (list)\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    # iterate\n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        # forward propagation\n",
        "        A, cost = forward_propagate(X, Y, w, b)\n",
        "        \n",
        "        # backward propagation\n",
        "        grads = backward_propagate(X, Y, A)\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update array of weights and scalar bias\n",
        "        w = w - learning_rate*dw\n",
        "        b = b -  learning_rate*db\n",
        "        \n",
        "        # save the costs (every 100th)\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 10 training examples\n",
        "        if print_cost:\n",
        "          if i % 10 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    # save optimized w and b in dictionary\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    # save dw and db\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6jxlyZK54BBe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now code a function to actually do our prediciton.  Remember, an activation >= 0.5 is a cat.  Else, it's not a cat."
      ]
    },
    {
      "metadata": {
        "id": "ZVIisVKTO1MB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Given a set of flattened images, predict their labels.\n",
        "    \n",
        "    input:   w               array of weights (num_pixels,1)\n",
        "             b               scalar bias (float)\n",
        "             X               array of flattened images (num_pixels,num_images)\n",
        "    output:  Y_prediction    array of predictions (num_images)\n",
        "    '''\n",
        "    \n",
        "    # get number of images\n",
        "    num_images = X.shape[1]\n",
        "    \n",
        "    # initialize prediction array\n",
        "    Y_prediction = np.zeros((1,num_images))\n",
        "    \n",
        "    # calculate activation (probability) for each image\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    \n",
        "    # make predictions\n",
        "    Y_prediction[A>=0.5] = 1\n",
        "    Y_prediction[A<0.5] = 0\n",
        "    \n",
        "    return Y_prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Yv7hPXd4mu1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now wrap everything up conveniently in a single function that orchestrates the optimization and reports diagnostics."
      ]
    },
    {
      "metadata": {
        "id": "BDD8laDbPSvJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 1000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    A wrapper for gradient descent.\n",
        "    \n",
        "    input:   X_train         array of flattened images for training (num_pixels,num_train)\n",
        "             Y_train         array of training labels (num_train)\n",
        "             X_test          array of flattened images for testing (num_pixels,num_test)\n",
        "             Y_test          array of testing labels (num_test)\n",
        "             num_iterations  number of iterations for optimization (scalar)\n",
        "             learning_rate   gradient multiplier (scalar)\n",
        "             print_cost      boolean controlling user feedback\n",
        "    output:  d               a dictionary of parameters and costs\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize\n",
        "    w, b = initialize_model(X_train.shape[0])\n",
        "    \n",
        "    # gradient descent with training set (optimization)\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # get w and b\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # predict on testing and training set\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # print errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    # save some parameters into a dictionary\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "owLPoNWI6ERm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's run everything."
      ]
    },
    {
      "metadata": {
        "id": "VvoESatcPjNM",
        "colab_type": "code",
        "outputId": "cedce2b4-46c6-46af-ba41-8ee87900f6ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3794
        }
      },
      "cell_type": "code",
      "source": [
        "d = model(train_set_x.T, labels_train, test_set_x.T, labels_test, num_iterations = 2000, learning_rate = 0.002, print_cost = True)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 10: 0.693859\n",
            "Cost after iteration 20: 0.664061\n",
            "Cost after iteration 30: 0.975437\n",
            "Cost after iteration 40: 0.665026\n",
            "Cost after iteration 50: 0.961135\n",
            "Cost after iteration 60: 0.674485\n",
            "Cost after iteration 70: 0.927890\n",
            "Cost after iteration 80: 0.694614\n",
            "Cost after iteration 90: 0.891936\n",
            "Cost after iteration 100: 0.720006\n",
            "Cost after iteration 110: 0.851959\n",
            "Cost after iteration 120: 0.748582\n",
            "Cost after iteration 130: 0.814113\n",
            "Cost after iteration 140: 0.770649\n",
            "Cost after iteration 150: 0.789634\n",
            "Cost after iteration 160: 0.777661\n",
            "Cost after iteration 170: 0.779076\n",
            "Cost after iteration 180: 0.775622\n",
            "Cost after iteration 190: 0.773822\n",
            "Cost after iteration 200: 0.771645\n",
            "Cost after iteration 210: 0.769639\n",
            "Cost after iteration 220: 0.767669\n",
            "Cost after iteration 230: 0.765758\n",
            "Cost after iteration 240: 0.763898\n",
            "Cost after iteration 250: 0.762086\n",
            "Cost after iteration 260: 0.760321\n",
            "Cost after iteration 270: 0.758598\n",
            "Cost after iteration 280: 0.756917\n",
            "Cost after iteration 290: 0.755274\n",
            "Cost after iteration 300: 0.753667\n",
            "Cost after iteration 310: 0.752095\n",
            "Cost after iteration 320: 0.750557\n",
            "Cost after iteration 330: 0.749049\n",
            "Cost after iteration 340: 0.747572\n",
            "Cost after iteration 350: 0.746123\n",
            "Cost after iteration 360: 0.744702\n",
            "Cost after iteration 370: 0.743307\n",
            "Cost after iteration 380: 0.741937\n",
            "Cost after iteration 390: 0.740592\n",
            "Cost after iteration 400: 0.739269\n",
            "Cost after iteration 410: 0.737969\n",
            "Cost after iteration 420: 0.736690\n",
            "Cost after iteration 430: 0.735432\n",
            "Cost after iteration 440: 0.734194\n",
            "Cost after iteration 450: 0.732976\n",
            "Cost after iteration 460: 0.731776\n",
            "Cost after iteration 470: 0.730594\n",
            "Cost after iteration 480: 0.729429\n",
            "Cost after iteration 490: 0.728281\n",
            "Cost after iteration 500: 0.727150\n",
            "Cost after iteration 510: 0.726035\n",
            "Cost after iteration 520: 0.724935\n",
            "Cost after iteration 530: 0.723850\n",
            "Cost after iteration 540: 0.722779\n",
            "Cost after iteration 550: 0.721723\n",
            "Cost after iteration 560: 0.720680\n",
            "Cost after iteration 570: 0.719651\n",
            "Cost after iteration 580: 0.718635\n",
            "Cost after iteration 590: 0.717631\n",
            "Cost after iteration 600: 0.716639\n",
            "Cost after iteration 610: 0.715660\n",
            "Cost after iteration 620: 0.714692\n",
            "Cost after iteration 630: 0.713736\n",
            "Cost after iteration 640: 0.712790\n",
            "Cost after iteration 650: 0.711856\n",
            "Cost after iteration 660: 0.710931\n",
            "Cost after iteration 670: 0.710017\n",
            "Cost after iteration 680: 0.709114\n",
            "Cost after iteration 690: 0.708219\n",
            "Cost after iteration 700: 0.707335\n",
            "Cost after iteration 710: 0.706459\n",
            "Cost after iteration 720: 0.705593\n",
            "Cost after iteration 730: 0.704736\n",
            "Cost after iteration 740: 0.703887\n",
            "Cost after iteration 750: 0.703047\n",
            "Cost after iteration 760: 0.702215\n",
            "Cost after iteration 770: 0.701391\n",
            "Cost after iteration 780: 0.700574\n",
            "Cost after iteration 790: 0.699766\n",
            "Cost after iteration 800: 0.698965\n",
            "Cost after iteration 810: 0.698172\n",
            "Cost after iteration 820: 0.697385\n",
            "Cost after iteration 830: 0.696606\n",
            "Cost after iteration 840: 0.695834\n",
            "Cost after iteration 850: 0.695068\n",
            "Cost after iteration 860: 0.694309\n",
            "Cost after iteration 870: 0.693556\n",
            "Cost after iteration 880: 0.692810\n",
            "Cost after iteration 890: 0.692070\n",
            "Cost after iteration 900: 0.691336\n",
            "Cost after iteration 910: 0.690608\n",
            "Cost after iteration 920: 0.689885\n",
            "Cost after iteration 930: 0.689169\n",
            "Cost after iteration 940: 0.688458\n",
            "Cost after iteration 950: 0.687752\n",
            "Cost after iteration 960: 0.687052\n",
            "Cost after iteration 970: 0.686357\n",
            "Cost after iteration 980: 0.685667\n",
            "Cost after iteration 990: 0.684982\n",
            "Cost after iteration 1000: 0.684303\n",
            "Cost after iteration 1010: 0.683628\n",
            "Cost after iteration 1020: 0.682957\n",
            "Cost after iteration 1030: 0.682292\n",
            "Cost after iteration 1040: 0.681631\n",
            "Cost after iteration 1050: 0.680974\n",
            "Cost after iteration 1060: 0.680322\n",
            "Cost after iteration 1070: 0.679675\n",
            "Cost after iteration 1080: 0.679031\n",
            "Cost after iteration 1090: 0.678392\n",
            "Cost after iteration 1100: 0.677757\n",
            "Cost after iteration 1110: 0.677125\n",
            "Cost after iteration 1120: 0.676498\n",
            "Cost after iteration 1130: 0.675875\n",
            "Cost after iteration 1140: 0.675256\n",
            "Cost after iteration 1150: 0.674640\n",
            "Cost after iteration 1160: 0.674028\n",
            "Cost after iteration 1170: 0.673420\n",
            "Cost after iteration 1180: 0.672815\n",
            "Cost after iteration 1190: 0.672214\n",
            "Cost after iteration 1200: 0.671616\n",
            "Cost after iteration 1210: 0.671021\n",
            "Cost after iteration 1220: 0.670430\n",
            "Cost after iteration 1230: 0.669843\n",
            "Cost after iteration 1240: 0.669258\n",
            "Cost after iteration 1250: 0.668677\n",
            "Cost after iteration 1260: 0.668099\n",
            "Cost after iteration 1270: 0.667524\n",
            "Cost after iteration 1280: 0.666952\n",
            "Cost after iteration 1290: 0.666383\n",
            "Cost after iteration 1300: 0.665817\n",
            "Cost after iteration 1310: 0.665254\n",
            "Cost after iteration 1320: 0.664694\n",
            "Cost after iteration 1330: 0.664136\n",
            "Cost after iteration 1340: 0.663582\n",
            "Cost after iteration 1350: 0.663030\n",
            "Cost after iteration 1360: 0.662481\n",
            "Cost after iteration 1370: 0.661934\n",
            "Cost after iteration 1380: 0.661390\n",
            "Cost after iteration 1390: 0.660849\n",
            "Cost after iteration 1400: 0.660311\n",
            "Cost after iteration 1410: 0.659775\n",
            "Cost after iteration 1420: 0.659241\n",
            "Cost after iteration 1430: 0.658710\n",
            "Cost after iteration 1440: 0.658181\n",
            "Cost after iteration 1450: 0.657655\n",
            "Cost after iteration 1460: 0.657131\n",
            "Cost after iteration 1470: 0.656610\n",
            "Cost after iteration 1480: 0.656090\n",
            "Cost after iteration 1490: 0.655573\n",
            "Cost after iteration 1500: 0.655059\n",
            "Cost after iteration 1510: 0.654546\n",
            "Cost after iteration 1520: 0.654036\n",
            "Cost after iteration 1530: 0.653528\n",
            "Cost after iteration 1540: 0.653022\n",
            "Cost after iteration 1550: 0.652518\n",
            "Cost after iteration 1560: 0.652017\n",
            "Cost after iteration 1570: 0.651517\n",
            "Cost after iteration 1580: 0.651019\n",
            "Cost after iteration 1590: 0.650524\n",
            "Cost after iteration 1600: 0.650030\n",
            "Cost after iteration 1610: 0.649539\n",
            "Cost after iteration 1620: 0.649049\n",
            "Cost after iteration 1630: 0.648562\n",
            "Cost after iteration 1640: 0.648076\n",
            "Cost after iteration 1650: 0.647592\n",
            "Cost after iteration 1660: 0.647110\n",
            "Cost after iteration 1670: 0.646630\n",
            "Cost after iteration 1680: 0.646152\n",
            "Cost after iteration 1690: 0.645675\n",
            "Cost after iteration 1700: 0.645200\n",
            "Cost after iteration 1710: 0.644727\n",
            "Cost after iteration 1720: 0.644256\n",
            "Cost after iteration 1730: 0.643787\n",
            "Cost after iteration 1740: 0.643319\n",
            "Cost after iteration 1750: 0.642853\n",
            "Cost after iteration 1760: 0.642389\n",
            "Cost after iteration 1770: 0.641926\n",
            "Cost after iteration 1780: 0.641465\n",
            "Cost after iteration 1790: 0.641006\n",
            "Cost after iteration 1800: 0.640548\n",
            "Cost after iteration 1810: 0.640092\n",
            "Cost after iteration 1820: 0.639638\n",
            "Cost after iteration 1830: 0.639185\n",
            "Cost after iteration 1840: 0.638734\n",
            "Cost after iteration 1850: 0.638284\n",
            "Cost after iteration 1860: 0.637836\n",
            "Cost after iteration 1870: 0.637389\n",
            "Cost after iteration 1880: 0.636944\n",
            "Cost after iteration 1890: 0.636500\n",
            "Cost after iteration 1900: 0.636057\n",
            "Cost after iteration 1910: 0.635617\n",
            "Cost after iteration 1920: 0.635177\n",
            "Cost after iteration 1930: 0.634739\n",
            "Cost after iteration 1940: 0.634303\n",
            "Cost after iteration 1950: 0.633868\n",
            "Cost after iteration 1960: 0.633434\n",
            "Cost after iteration 1970: 0.633002\n",
            "Cost after iteration 1980: 0.632571\n",
            "Cost after iteration 1990: 0.632141\n",
            "train accuracy: 68.28078524687686 %\n",
            "test accuracy: 59.36920222634509 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0ugkgwa6uRo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Your Assignment\n",
        "In cells below (or in a Python script), include the following:\n",
        "\n",
        "*   Try to improve the prediction.  To do this, you can modify the training and testing sets (perhaps we need more non-cats in training).  You can try a different activation function.  You can obviously try different numbers of iterations.  Be creative.  Report everything you have tried.  For each attempt include code, output, and a discussion.\n",
        "*   Derive the derivatives of the cost function, `dw` and `db`.\n",
        "*  In a couple of paragraphs, summarize what you have learned.  Discuss limitations of the approach.  Image recognition is not done this way in production environments!  Discuss how it is done by data scientists today.  Google around.  Some of you will be using these methods for your FCC.\n",
        "\n",
        "\n"
      ]
    }
  ]
}